[
  {
    "objectID": "projects/cnn_model.html",
    "href": "projects/cnn_model.html",
    "title": "CNN Model Project",
    "section": "",
    "text": "In this project, we build a Convolutional Neural Network (CNN) to classify images from the CIFAR-10 dataset using R, diverging from the common trend of using Python for machine learning. This CNN implementation serves as a crucial foundation before exploring more complex RNN models. The CIFAR-10 dataset contains 60,000 32x32 color images categorized into 10 classes, with 6,000 images per class. Our objective is to provide a systematic approach to constructing, training, and applying the model for image classification, setting up essential skills for advanced neural network architectures like RNN and LSTM. This could be considered a short tutorial that covers every step of the process, including dataset loading, model creation, training, and evaluation."
  },
  {
    "objectID": "projects/cnn_model.html#model-1-evaluation",
    "href": "projects/cnn_model.html#model-1-evaluation",
    "title": "CNN Model Project",
    "section": "3.1 Model 1 Evaluation",
    "text": "3.1 Model 1 Evaluation\nThe plot demonstrate the training results. The last part of the code represent the plot generated of the model performance during the training. By observing the plot, we detect a sign of overfitting. A situation where model performs exceptionally well on training data but poorly on new data. If the model’s performance on the validation set begins to drop while continuing to improve on the training set, then this is a clear indication of overfittning. Now, with this plot, we can adjust our current model to mitigate the overfitting issue.\n\n\nCode\n# Model 2\nmodel.2 &lt;- keras_model_sequential() %&gt;%\n\n# First conv layer\n   layer_conv_2d(filters=32, kernel_size = c(3, 3), padding=\"same\", input_shape = c(32, 32, 3), activation = 'relu') %&gt;%\n   layer_batch_normalization() %&gt;%\n\n# Second conv layer\n   layer_conv_2d(filters=32, kernel_size = c(3, 3), padding=\"same\", activation = 'relu') %&gt;%\n   layer_batch_normalization() %&gt;%\n   layer_max_pooling_2d(pool_size = c(2,2)) %&gt;%\n   layer_dropout(0.2) %&gt;%\n\n# Third and fourth conv layer\n   layer_conv_2d(filters=64, kernel_size = c(3, 3), padding=\"same\", activation = 'relu') %&gt;%\n   layer_batch_normalization() %&gt;%\n   layer_conv_2d(filters=64, kernel_size = c(3, 3), padding=\"same\", activation = 'relu') %&gt;%\n   layer_batch_normalization() %&gt;%\n   layer_max_pooling_2d(pool_size = c(2,2)) %&gt;%\n   layer_dropout(0.3) %&gt;%\n\n# Fifth and sixth conv layer\n   layer_conv_2d(filters=128, kernel_size = c(3, 3), padding=\"same\", activation = 'relu') %&gt;%\n   layer_batch_normalization() %&gt;%\n   layer_conv_2d(filters=128, kernel_size = c(3, 3), padding=\"same\", activation = 'relu') %&gt;%\n   layer_batch_normalization() %&gt;%\n   layer_max_pooling_2d(pool_size = c(2,2)) %&gt;%\n   layer_dropout(0.4) %&gt;%\n\n# Flatten layer\n   layer_flatten() %&gt;%\n\n# Dense layers\n   layer_dense(units = 256, activation = 'relu') %&gt;%\n   layer_dense(units = 10, activation = 'softmax')\n\nmodel.2 %&gt;% summary()\n\n\nModel: \"sequential_1\"\n________________________________________________________________________________\n Layer (type)                  Output Shape               Param #    Trainable  \n================================================================================\n conv2d_9 (Conv2D)             (None, 32, 32, 32)         896        Y          \n batch_normalization_9 (Batch  (None, 32, 32, 32)         128        Y          \n Normalization)                                                                 \n conv2d_8 (Conv2D)             (None, 32, 32, 32)         9248       Y          \n batch_normalization_8 (Batch  (None, 32, 32, 32)         128        Y          \n Normalization)                                                                 \n max_pooling2d_4 (MaxPooling2  (None, 16, 16, 32)         0          Y          \n D)                                                                             \n dropout_4 (Dropout)           (None, 16, 16, 32)         0          Y          \n conv2d_7 (Conv2D)             (None, 16, 16, 64)         18496      Y          \n batch_normalization_7 (Batch  (None, 16, 16, 64)         256        Y          \n Normalization)                                                                 \n conv2d_6 (Conv2D)             (None, 16, 16, 64)         36928      Y          \n batch_normalization_6 (Batch  (None, 16, 16, 64)         256        Y          \n Normalization)                                                                 \n max_pooling2d_3 (MaxPooling2  (None, 8, 8, 64)           0          Y          \n D)                                                                             \n dropout_3 (Dropout)           (None, 8, 8, 64)           0          Y          \n conv2d_5 (Conv2D)             (None, 8, 8, 128)          73856      Y          \n batch_normalization_5 (Batch  (None, 8, 8, 128)          512        Y          \n Normalization)                                                                 \n conv2d_4 (Conv2D)             (None, 8, 8, 128)          147584     Y          \n batch_normalization_4 (Batch  (None, 8, 8, 128)          512        Y          \n Normalization)                                                                 \n max_pooling2d_2 (MaxPooling2  (None, 4, 4, 128)          0          Y          \n D)                                                                             \n dropout_2 (Dropout)           (None, 4, 4, 128)          0          Y          \n flatten_1 (Flatten)           (None, 2048)               0          Y          \n dense_3 (Dense)               (None, 256)                524544     Y          \n dense_2 (Dense)               (None, 10)                 2570       Y          \n================================================================================\nTotal params: 815914 (3.11 MB)\nTrainable params: 815018 (3.11 MB)\nNon-trainable params: 896 (3.50 KB)\n________________________________________________________________________________\n\n\nSince we observed overfitting issues in the plot for our initial model, we decided to adjust the capacity of our model, which we called model 2. We increased the complexity of the model by adding more convolutional layers and increasing the number of filters in the later layers. This approach was taken to potentially improve the model’s ability to learn more complex features from the data. However, recognizing that deeper networks can be prone to memorizing patterns rather than learning generalizable features, we also implemented additional measures to combat potential overfitting. Specifically, we added more dropout layers throughout the network. These dropout layers act as a form of regularization, randomly deactivating a portion of neurons during training, which helps prevent the model from relying too heavily on any specific features.\nOur main goal with model 2 is to get better accuracy than the first model, while still avoiding overfitting. We’re trying to find a balance between making the model smart enough to learn complex patterns, but not so complex that it just memorizes the training data.\n\n\nCode\n# Compile model.2\nmodel.2 %&gt;% compile(\n  optimizer = \"adam\",\n  loss = 'categorical_crossentropy',\n  metrics = 'accuracy'\n)\n\n\nThe compiling set up for the model remains the same as the previous model, and we intent to keep it consistent across this project. Yet, we may adjust it if needed to optimize the model performance. Considering our computational limitation, we believe this set up is the optimal one between learning speed and stability. Additionally, by maintaining this consistency, we can more effectively compare the results of different model architectures.\n\n\nCode\n# Train Model 2\ntrain.2 = model.2 %&gt;% fit(\n  t.images, t.labels,\n  epochs = 50,\n  batch_size = 32,\n  validation_data = list(test.images, test.labels),\n  shuffle = TRUE\n)\n\n# Saving\npng(\"model_2_training_plot2.png\")\nplot(train.2)\ndev.off()\n\nsave_model_hdf5(model.2, filepath = \"cnn_model_2.h5\")\n\nevaluation_result = model.2 %&gt;% evaluate(test.images, test.labels)\nsaveRDS(evaluation_result, file = \"model_2_evaluation2.rds\")\n\n\nWe are taking the same steps as in the initial model training as it align with our computation resource but also we want to ensure consistency and comparability across this project. Despite the architectural changes in model 2, maintaining these consistent training procedures allows us to isolate the effects of our model modifications. Any differences in performance can then be more confidently attributed to the changes in model structure rather than variations in the training process.\n\n\nCode\n# Uploading \nknitr::include_graphics(\"images/model_2_training_plot.png\")\n\n\n\n\n\nCode\nevaluation_result = readRDS(\"images/model_2_evaluation2.rds\")\nprint(evaluation_result)\n\n\n    loss accuracy \n  0.5543   0.8469"
  },
  {
    "objectID": "projects/cnn_model.html#model-2-evaluation",
    "href": "projects/cnn_model.html#model-2-evaluation",
    "title": "CNN Model Project",
    "section": "3.2 Model 2 Evaluation",
    "text": "3.2 Model 2 Evaluation\nThe plot here are the results of model 2 training, and comparing it to model 1 it reveals some notable contrasts. In the case of model 1, we observe signs of overfitting. We can see that the training loss decreases steadily, while the validation loss begins to rise after an initial dip. At the same time, training accuracy climbs to its highest level, in the mean time validation accuracy stagnates and even declines slightly, signaling poor generalization. This type of behavior often suggests that the model relies on memorization, basically memorizing training data rather than learning patterns that extend to unseen data. The result of evaluation metrics back this up, with model 1 achieving a validation loss of 0.798 and an accuracy of 81.73%, reflecting its limited effectiveness on new data.\nModel 2, however, shows a slight improvement, specifically if we observe the decreased gap in the accuracy part between the two plots. Training and validation losses both decrease smoothly, with validation loss stabilizing at a lower point. Validation accuracy also shows consistent growth, closely tracking training accuracy, which signals stronger generalization. The evaluation results confirm these gains, as model 2 achieves a reduced validation loss of 0.554 and a higher accuracy of 84.6%. Regularization techniques, like dropout, were key to this improvement. As a result, model 2 is not only better at generalizing but also more dependable for real-world tasks.\nAlthough model 2 shows significant improvements over model 1, there is still room for further enhancement.The validation loss, while reduced, remains higher than the training loss, indicating a potential gap in generalization. With that in mind, There is different techniques that could be applied to refine our current model. We could say that we need to improve the model, and the best approach we came up with was implementing data augmentation to artificially expand the training data and its diversity.\n\n\nCode\n# Now adding Data Augmentation for model.3\n\n# Data augmentation prep\ndatagen = image_data_generator(\n  rotation_range = 30,\n  width_shift_range = 0.1,  \n  height_shift_range = 0.1, \n  shear_range = 0.2,        \n  zoom_range = 0.2,        \n  horizontal_flip = TRUE,   \n  fill_mode = \"nearest\" )\n\n# data augmentation generator to training data\ndatagen %&gt;% fit_image_data_generator(t.images)\n\n\nData augmentation is an approach that is widely used to create new versions of already existing images by manipulating their features. Simply put, it involves applying changes to existing images through techniques like rotating, flipping, cropping, or adjusting brightness. This can be beneficial because it helps the model learn better by exposing it to different variations of the images without needing more real images. Our believe is that this could improve the model’s performance, especially when we have limited data.\n\n\nCode\n# New Model (model.3) with the same architecture as model.2\nmodel.3 &lt;- keras_model_sequential() %&gt;%\n   \n# First conv layer\n   layer_conv_2d(filters=32, kernel_size = c(3, 3), padding=\"same\", input_shape = c(32, 32, 3), activation = 'relu') %&gt;%\n   layer_batch_normalization() %&gt;%\n\n# Second conv layer\n   layer_conv_2d(filters=32, kernel_size = c(3, 3), padding=\"same\", activation = 'relu') %&gt;%\n   layer_batch_normalization() %&gt;%\n   layer_max_pooling_2d(pool_size = c(2,2)) %&gt;%\n   layer_dropout(0.25) %&gt;%\n\n# Third and fourth conv layer\n   layer_conv_2d(filters=64, kernel_size = c(3, 3), padding=\"same\", activation = 'relu') %&gt;%\n   layer_batch_normalization() %&gt;%\n   layer_conv_2d(filters=64, kernel_size = c(3, 3), padding=\"same\", activation = 'relu') %&gt;%\n   layer_batch_normalization() %&gt;%\n   layer_max_pooling_2d(pool_size = c(2,2)) %&gt;%\n   layer_dropout(0.35) %&gt;%\n\n# Fifth and sixth conv layer\n   layer_conv_2d(filters=128, kernel_size = c(3, 3), padding=\"same\", activation = 'relu') %&gt;%\n   layer_batch_normalization() %&gt;%\n   layer_conv_2d(filters=128, kernel_size = c(3, 3), padding=\"same\", activation = 'relu') %&gt;%\n   layer_batch_normalization() %&gt;%\n   layer_max_pooling_2d(pool_size = c(2,2)) %&gt;%\n   layer_dropout(0.45) %&gt;%\n\n# Flatten layer\n   layer_flatten() %&gt;%\n\n# Dense layers\n   layer_dense(units = 256, activation = 'relu') %&gt;%\n   layer_dense(units = 10, activation = 'softmax')\n\nmodel.3 %&gt;% summary()\n\n\nModel: \"sequential_2\"\n________________________________________________________________________________\n Layer (type)                  Output Shape               Param #    Trainable  \n================================================================================\n conv2d_15 (Conv2D)            (None, 32, 32, 32)         896        Y          \n batch_normalization_15 (Batc  (None, 32, 32, 32)         128        Y          \n hNormalization)                                                                \n conv2d_14 (Conv2D)            (None, 32, 32, 32)         9248       Y          \n batch_normalization_14 (Batc  (None, 32, 32, 32)         128        Y          \n hNormalization)                                                                \n max_pooling2d_7 (MaxPooling2  (None, 16, 16, 32)         0          Y          \n D)                                                                             \n dropout_7 (Dropout)           (None, 16, 16, 32)         0          Y          \n conv2d_13 (Conv2D)            (None, 16, 16, 64)         18496      Y          \n batch_normalization_13 (Batc  (None, 16, 16, 64)         256        Y          \n hNormalization)                                                                \n conv2d_12 (Conv2D)            (None, 16, 16, 64)         36928      Y          \n batch_normalization_12 (Batc  (None, 16, 16, 64)         256        Y          \n hNormalization)                                                                \n max_pooling2d_6 (MaxPooling2  (None, 8, 8, 64)           0          Y          \n D)                                                                             \n dropout_6 (Dropout)           (None, 8, 8, 64)           0          Y          \n conv2d_11 (Conv2D)            (None, 8, 8, 128)          73856      Y          \n batch_normalization_11 (Batc  (None, 8, 8, 128)          512        Y          \n hNormalization)                                                                \n conv2d_10 (Conv2D)            (None, 8, 8, 128)          147584     Y          \n batch_normalization_10 (Batc  (None, 8, 8, 128)          512        Y          \n hNormalization)                                                                \n max_pooling2d_5 (MaxPooling2  (None, 4, 4, 128)          0          Y          \n D)                                                                             \n dropout_5 (Dropout)           (None, 4, 4, 128)          0          Y          \n flatten_2 (Flatten)           (None, 2048)               0          Y          \n dense_5 (Dense)               (None, 256)                524544     Y          \n dense_4 (Dense)               (None, 10)                 2570       Y          \n================================================================================\nTotal params: 815914 (3.11 MB)\nTrainable params: 815018 (3.11 MB)\nNon-trainable params: 896 (3.50 KB)\n________________________________________________________________________________\n\n\nTo improve our model further, we named this new version model 3 for simplicity reasons. This model maintains the same design as model 2, just to be able to implement the data augmentation technique to improve the models performance. By keeping the architecture identical to model 2, we can directly observe the impact of data augmentation on the model’s ability to learn and generalize. But the implementation of data augmentation do not happen until the training phase.\n\n\nCode\n# Compile model.3 (same as model.2)\nmodel.3 %&gt;% compile(\n  optimizer = \"adam\",\n  loss = 'categorical_crossentropy',\n  metrics = 'accuracy'\n)\n\n\nThe compilation process for this part is identical to that of Model 2. We intentionally maintained the same compilation settings to make sure a fair comparison between the two models. This will keep the architecture and compilation parameters consistent, we can more accurately observe and analyze any differences in performance.\n\n\nCode\n# Train Model 3 with augmented data\ngenerator = flow_images_from_data(t.images, t.labels, datagen, batch_size = 32)\ntrain.3 = model.3 %&gt;% fit(\n  generator,\n  steps_per_epoch = as.integer(nrow(t.images) / 32),\n  epochs = 50,\n  validation_data = list(test.images, test.labels),\n  shuffle = TRUE\n)\n\n# Saving\npng(\"model_3_training_plot3.png\")\nplot(train.3)\ndev.off()\n\nsave_model_hdf5(model.3, filepath = \"cnn_model_3.h5\")\n\n# Saving the Results\nevaluation_result =  model.3 %&gt;% evaluate(test.images, test.labels)\nsaveRDS(evaluation_result, file = \"model_3_evaluation3.rds\")\n\n\nIn this script, we train model 3 with augmented data to improve the model generalization capability. This can be achieved by first creating a data generator that applies real-time augmentations to the input images (t.images) and their corresponding labels (t.labels) during training, and then feeding the augmented data directly into the model. Similarly to the previous set up, the model trains for 50 epochs in batches of 32, with validation data tracking its performance. A plot of the training progress is saved as model_3_training_plot3.png, and the trained model is stored in HDF5 format (cnn_model_3.h5) for later use.\n\n\nCode\n# The results for Model 3\nknitr::include_graphics(\"images/model_3_training_plot3.png\")\n\n\n\n\n\nCode\n# Load the Saved Evaluation Result\nevaluation_result &lt;- readRDS(\"images/model_3_evaluation3.rds\")\nprint(evaluation_result)\n\n\n    loss accuracy \n 0.47224  0.86223"
  },
  {
    "objectID": "projects/cnn_model.html#model-3-evaluation",
    "href": "projects/cnn_model.html#model-3-evaluation",
    "title": "CNN Model Project",
    "section": "3.3 Model 3 Evaluation",
    "text": "3.3 Model 3 Evaluation\nThe results of Model 3 training are illustrated in this plot. The plot shows clear progress, especially when compared to the plot of Model 2. Training and validation losses decrease consistently, with validation loss stabilizing at a noticeably lower value. Validation accuracy steadily improves and aligns more closely with training accuracy, reflecting stronger generalization. The evaluation results for Model 3 further emphasize these gains, achieving a validation loss of 0.4722 and a higher accuracy of 86.23%, compared to Model 2’s accuracy of 84.6%%. These improvements are largely due to the introduction of data augmentation, which diversified the dataset and allowed the model to better handle unseen data.\nThere are several ways to further optimize the model and enhance its performance. One commonly used approach is fine-tuning the hyperparameters. This is a process that mostly involves experimenting with different aspects of the learning process, such as adjusting the learning rate, modifying the number of epochs, or testing various batch sizes to find the best configuration. For example, using a smaller learning rate allows the model to take smaller, more precise steps toward an optimal solution, reducing the risk of missing key patterns. On the other hand, a larger learning rate can speed up training but may overshoot ideal values, leading to sub-optimal results. Similarly, adjusting the number of epochs can help the model learn more thoroughly, while experimenting with batch sizes affects computational efficiency and gradient stability.\nWe believe these strategies are valuable if our main focus lies in optimizing our CNN model, but they require careful experimentation and additional resources that we currently do not possess. Unfortunately, this falls beyond the scope of the current project. However, these methods remain valuable options for future refinement if more time or resources become available."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Home",
    "section": "",
    "text": "My name is Mustafa, and the things I value most are honesty, generosity, and hard work. These are the principles that keep me grounded—in both my personal life and the way I approach my work. For some, they may seem like abstract concepts, but for me, they shape how I interact with people and approach my day to day challenges .\nFootball has shaped a big part of who I am. I spent almost two decades playing the game, including some time at a semi-elite level. But it wasn’t just about winning matches. It taught me how to lead a team, really listen to others, and find ways to bring people together. And maybe the biggest lesson? Learning to stick with something, no matter how tough it gets.\nThose lessons didn’t stay on the field. They show up in my day-to-day life. Staying active, picking up a new book, or trying something completely new—it’s all part of how I keep learning and growing.\n:::"
  },
  {
    "objectID": "about.html#who-am-i",
    "href": "about.html#who-am-i",
    "title": "Home",
    "section": "",
    "text": "My name is Mustafa, and the things I value most are honesty, generosity, and hard work. These are the principles that keep me grounded—in both my personal life and the way I approach my work. For some, they may seem like abstract concepts, but for me, they shape how I interact with people and approach my day to day challenges .\nFootball has shaped a big part of who I am. I spent almost two decades playing the game, including some time at a semi-elite level. But it wasn’t just about winning matches. It taught me how to lead a team, really listen to others, and find ways to bring people together. And maybe the biggest lesson? Learning to stick with something, no matter how tough it gets.\nThose lessons didn’t stay on the field. They show up in my day-to-day life. Staying active, picking up a new book, or trying something completely new—it’s all part of how I keep learning and growing."
  },
  {
    "objectID": "about.html#experiences-that-shaped-me",
    "href": "about.html#experiences-that-shaped-me",
    "title": "Home",
    "section": "Experiences that shaped me",
    "text": "Experiences that shaped me\n\nOne of the key moment that shaped me, I still remember it to this day. As an introvert, selling was definitely not my strongest skill. Putting myself out there to sell goods and talk to strangers wasn’t easy. At first, I stumbled. It felt awkward and way out of my depth. But I stuck with it, and at last, things started to click and became relatively easier. I became more confident, learned to connect with people on a deeper level, and realized how important it is to just listen sometimes."
  },
  {
    "objectID": "about.html#education-and-goals",
    "href": "about.html#education-and-goals",
    "title": "Home",
    "section": "Education and goals",
    "text": "Education and goals\nI have never considered myself a ‘math person.’ But you know how sometimes curiosity gets the better of you? That’s what happened to me with math. Since then, I have become fascinated by the logic and precision of math and statistics, which certainly motivated me to earn a Master’s degree in Economics, a path I believe will benefit my long-term goal of becoming a risk analyst. Right now, I’m expanding my knowledge by building my foundation through courses in actuarial mathematics and stochastic processes"
  },
  {
    "objectID": "projects/survival_analysis.html",
    "href": "projects/survival_analysis.html",
    "title": "Project Coming Soon",
    "section": "",
    "text": "Project Coming Soon\n\nThe project will be available shortly—stay tuned for updates."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Project portfolio",
    "section": "",
    "text": "Explore the world of information"
  },
  {
    "objectID": "index.html#my-data-journey",
    "href": "index.html#my-data-journey",
    "title": "Project portfolio",
    "section": "My Data Journey",
    "text": "My Data Journey\nWelcome! I’m Mustafa, and my journey in data analysis began with a fundamental curiosity: Why is some data complex to uncover, and how can simplifying this complexity change the narratives that reshape our everyday experiences?\nWhat if the image below wasn’t just a view of Earth at night, but a reflection of the complexity of data? Each light could be a piece of information, scattered and interconnected, waiting to tell a story. How much could we uncover just by taking a closer look? Data can feel overwhelming, like staring at all these lights for the first time, but once we start making sense of it, patterns and insights begin to emerge. What do you see in this image? What story would you start to tell?"
  },
  {
    "objectID": "index.html#over-the-last-couple-of-months-ive-worked-on-some-interesting-side-projects",
    "href": "index.html#over-the-last-couple-of-months-ive-worked-on-some-interesting-side-projects",
    "title": "Project portfolio",
    "section": "Over the last couple of months, I’ve worked on some interesting side projects:",
    "text": "Over the last couple of months, I’ve worked on some interesting side projects:\n\n\n\n\nCNN Model Project\n\n\n\n\n\n\nSurvival Analysis"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Project portfolio",
    "section": "About me",
    "text": "About me"
  },
  {
    "objectID": "projects/predictive_risk_modeling.html#dataset",
    "href": "projects/predictive_risk_modeling.html#dataset",
    "title": "Predictive Risk Modeling for Auto Insurance: Claim Likelihood Estimation",
    "section": "",
    "text": "Code\n# Uploading  libraries\nlibrary(pacman)\np_load(tidyverse, rio, mice, corrr, summarytools, caret, naniar, broom, knitr, kableExtra, pROC, DALEX, modelsummary,ggtext,shapr,DT, shinydashboard,shiny,plotly, PRROC, rsconnect)\n\n\n\n\n\n\nCode\ndata_car = import(\"/Users/Brook/Downloads/Car_Insurance_Claim.csv\")\nhead(data_car, 5)\n\n\n      ID   AGE GENDER     RACE DRIVING_EXPERIENCE   EDUCATION        INCOME\n1 569520   65+ female majority               0-9y high school   upper class\n2 750365 16-25   male majority               0-9y        none       poverty\n3 199901 16-25 female majority               0-9y high school working class\n4 478866 16-25   male majority               0-9y  university working class\n5 731664 26-39   male majority             10-19y        none working class\n  CREDIT_SCORE VEHICLE_OWNERSHIP VEHICLE_YEAR MARRIED CHILDREN POSTAL_CODE\n1    0.6290273                 1   after 2015       0        1       10238\n2    0.3577571                 0  before 2015       0        0       10238\n3    0.4931458                 1  before 2015       0        0       10238\n4    0.2060129                 1  before 2015       0        1       32765\n5    0.3883659                 1  before 2015       0        0       32765\n  ANNUAL_MILEAGE VEHICLE_TYPE SPEEDING_VIOLATIONS DUIS PAST_ACCIDENTS OUTCOME\n1          12000        sedan                   0    0              0       0\n2          16000        sedan                   0    0              0       1\n3          11000        sedan                   0    0              0       0\n4          11000        sedan                   0    0              0       0\n5          12000        sedan                   2    0              1       1\n\n\n\n\nCode\n# Detailed summary of the missing values, counts and percentage, data type of the predictors\n\ndfSummary(data_car) \n\n\nData Frame Summary  \ndata_car  \nDimensions: 10000 x 19  \nDuplicates: 0  \n\n--------------------------------------------------------------------------------------------------------------------------------\nNo   Variable              Stats / Values                    Freqs (% of Valid)      Graph                  Valid      Missing  \n---- --------------------- --------------------------------- ----------------------- ---------------------- ---------- ---------\n1    ID                    Mean (sd) : 500521.9 (290030.8)   10000 distinct values   : . : . : : : . : :    10000      0        \n     [integer]             min &lt; med &lt; max:                                          : : : : : : : : : :    (100.0%)   (0.0%)   \n                           101 &lt; 501777 &lt; 999976                                     : : : : : : : : : :                        \n                           IQR (CV) : 504336 (0.6)                                   : : : : : : : : : :                        \n                                                                                     : : : : : : : : : :                        \n\n2    AGE                   1. 16-25                          2016 (20.2%)            IIII                   10000      0        \n     [character]           2. 26-39                          3063 (30.6%)            IIIIII                 (100.0%)   (0.0%)   \n                           3. 40-64                          2931 (29.3%)            IIIII                                      \n                           4. 65+                            1990 (19.9%)            III                                        \n\n3    GENDER                1. female                         5010 (50.1%)            IIIIIIIIII             10000      0        \n     [character]           2. male                           4990 (49.9%)            IIIIIIIII              (100.0%)   (0.0%)   \n\n4    RACE                  1. majority                       9012 (90.1%)            IIIIIIIIIIIIIIIIII     10000      0        \n     [character]           2. minority                        988 ( 9.9%)            I                      (100.0%)   (0.0%)   \n\n5    DRIVING_EXPERIENCE    1. 0-9y                           3530 (35.3%)            IIIIIII                10000      0        \n     [character]           2. 10-19y                         3299 (33.0%)            IIIIII                 (100.0%)   (0.0%)   \n                           3. 20-29y                         2119 (21.2%)            IIII                                       \n                           4. 30y+                           1052 (10.5%)            II                                         \n\n6    EDUCATION             1. high school                    4157 (41.6%)            IIIIIIII               10000      0        \n     [character]           2. none                           1915 (19.1%)            III                    (100.0%)   (0.0%)   \n                           3. university                     3928 (39.3%)            IIIIIII                                    \n\n7    INCOME                1. middle class                   2138 (21.4%)            IIII                   10000      0        \n     [character]           2. poverty                        1814 (18.1%)            III                    (100.0%)   (0.0%)   \n                           3. upper class                    4336 (43.4%)            IIIIIIII                                   \n                           4. working class                  1712 (17.1%)            III                                        \n\n8    CREDIT_SCORE          Mean (sd) : 0.5 (0.1)             9018 distinct values              : .          9018       982      \n     [numeric]             min &lt; med &lt; max:                                                  : : :          (90.2%)    (9.8%)   \n                           0.1 &lt; 0.5 &lt; 1                                                   : : : :                              \n                           IQR (CV) : 0.2 (0.3)                                          . : : : : .                            \n                                                                                       . : : : : : : .                          \n\n9    VEHICLE_OWNERSHIP     Min  : 0                          0 : 3030 (30.3%)        IIIIII                 10000      0        \n     [numeric]             Mean : 0.7                        1 : 6970 (69.7%)        IIIIIIIIIIIII          (100.0%)   (0.0%)   \n                           Max  : 1                                                                                             \n\n10   VEHICLE_YEAR          1. after 2015                     3033 (30.3%)            IIIIII                 10000      0        \n     [character]           2. before 2015                    6967 (69.7%)            IIIIIIIIIIIII          (100.0%)   (0.0%)   \n\n11   MARRIED               Min  : 0                          0 : 5018 (50.2%)        IIIIIIIIII             10000      0        \n     [numeric]             Mean : 0.5                        1 : 4982 (49.8%)        IIIIIIIII              (100.0%)   (0.0%)   \n                           Max  : 1                                                                                             \n\n12   CHILDREN              Min  : 0                          0 : 3112 (31.1%)        IIIIII                 10000      0        \n     [numeric]             Mean : 0.7                        1 : 6888 (68.9%)        IIIIIIIIIIIII          (100.0%)   (0.0%)   \n                           Max  : 1                                                                                             \n\n13   POSTAL_CODE           Mean (sd) : 19864.5 (18915.6)     10238 : 6940 (69.4%)    IIIIIIIIIIIII          10000      0        \n     [integer]             min &lt; med &lt; max:                  21217 :  120 ( 1.2%)                           (100.0%)   (0.0%)   \n                           10238 &lt; 10238 &lt; 92101             32765 : 2456 (24.6%)    IIII                                       \n                           IQR (CV) : 22527 (1)              92101 :  484 ( 4.8%)                                               \n\n14   ANNUAL_MILEAGE        Mean (sd) : 11697 (2818.4)        21 distinct values              :              9043       957      \n     [numeric]             min &lt; med &lt; max:                                                : : :            (90.4%)    (9.6%)   \n                           2000 &lt; 12000 &lt; 22000                                            : : :                                \n                           IQR (CV) : 4000 (0.2)                                         : : : : :                              \n                                                                                       . : : : : : .                            \n\n15   VEHICLE_TYPE          1. sedan                          9523 (95.2%)            IIIIIIIIIIIIIIIIIII    10000      0        \n     [character]           2. sports car                      477 ( 4.8%)                                   (100.0%)   (0.0%)   \n\n16   SPEEDING_VIOLATIONS   Mean (sd) : 1.5 (2.2)             21 distinct values      :                      10000      0        \n     [integer]             min &lt; med &lt; max:                                          :                      (100.0%)   (0.0%)   \n                           0 &lt; 0 &lt; 22                                                :                                          \n                           IQR (CV) : 2 (1.5)                                        :                                          \n                                                                                     : : .                                      \n\n17   DUIS                  Mean (sd) : 0.2 (0.6)             0 : 8118 (81.2%)        IIIIIIIIIIIIIIII       10000      0        \n     [integer]             min &lt; med &lt; max:                  1 : 1470 (14.7%)        II                     (100.0%)   (0.0%)   \n                           0 &lt; 0 &lt; 6                         2 :  331 ( 3.3%)                                                   \n                           IQR (CV) : 0 (2.3)                3 :   68 ( 0.7%)                                                   \n                                                             4 :   10 ( 0.1%)                                                   \n                                                             5 :    2 ( 0.0%)                                                   \n                                                             6 :    1 ( 0.0%)                                                   \n\n18   PAST_ACCIDENTS        Mean (sd) : 1.1 (1.7)             15 distinct values      :                      10000      0        \n     [integer]             min &lt; med &lt; max:                                          :                      (100.0%)   (0.0%)   \n                           0 &lt; 0 &lt; 15                                                :                                          \n                           IQR (CV) : 2 (1.6)                                        :                                          \n                                                                                     : :   .                                    \n\n19   OUTCOME               Min  : 0                          0 : 6867 (68.7%)        IIIIIIIIIIIII          10000      0        \n     [numeric]             Mean : 0.3                        1 : 3133 (31.3%)        IIIIII                 (100.0%)   (0.0%)   \n                           Max  : 1                                                                                             \n--------------------------------------------------------------------------------------------------------------------------------\n\n\nWe can see that data has been imported correctly, and the quality of the data has been thoroughly examined. The dataset consists of 10,000 drivers, where most of the variables appear to be clean, except for CREDIT_SCORE and ANNUAL_MILEAGE, each with around 10% missing values. This is not necessarily a bad thing, as the issue can be addressed using imputation. A common technique to replace missing values with estimated or substituted values. Imputation not only completes the dataset but also helps mitigate potential biases and gaps in information before conducting our analysis. Since the proportion of missing values is relatively small, we will use median imputation, which is a widely accepted practice in the field.\nOur target variable (OUTCOME) shows a moderate class imbalance (68.7% no claims vs. 31.3% claims), which we will address later using weighted GLM to mitigate the issue associated with such imbalance. Looking at key predictors such as VEHICLE_TYPE (4.8% high-risk sports cars), AGE (20.2% young drivers), and PAST_ACCIDENTS (maximum of 15 accidents), we can observe some risk patterns. Furthermore, we are dropping ID variable as it has no predictive value, as for the POSTAL_CODE variable, we are dropping it as well because it shows low variability (69.4%) and would likely add noise without contributing to a meaningful predictive capability. Especially considering that real-world postal codes often correlate with risk factors, such as accident-prone areas\nBased on the data review, this are the steps that are needed before producing our model, we need to handle the missing values, do something about outlier and lastly managing class imbalance for the target variable.\n\n\n\n\n\nCode\n# Dropping POSTAL_CODE and ID Variable \ndata_car = data_car[, c(-1, -13)]\n\n# Visuals over missing values in bar plot  \ngg_miss_var(data_car) + geom_col(aes(y = n_miss, fill = n_miss &gt; 0), color = \"black\", linewidth = 0.3) +  labs(subtitle = \"CREDIT_SCORE and ANNUAL_MILEAGE have missing values (~10%)\"\n  ) +\n  theme_classic(base_size = 12)\n\n\n\n\n\n\n\nCode\n# Imputing, using median \ndata_car  =  data_car %&gt;%  \n  mutate(  \n    CREDIT_SCORE = ifelse(is.na(CREDIT_SCORE), median(CREDIT_SCORE, na.rm = TRUE), CREDIT_SCORE),  \n    ANNUAL_MILEAGE = ifelse(is.na(ANNUAL_MILEAGE), median(ANNUAL_MILEAGE, na.rm = TRUE), ANNUAL_MILEAGE) )  \n\n# Checking if there's missing values in each variable\ncolSums(is.na(data_car))\n\n\n                AGE              GENDER                RACE  DRIVING_EXPERIENCE \n                  0                   0                   0                   0 \n          EDUCATION              INCOME        CREDIT_SCORE   VEHICLE_OWNERSHIP \n                  0                   0                   0                   0 \n       VEHICLE_YEAR             MARRIED            CHILDREN      ANNUAL_MILEAGE \n                  0                   0                   0                   0 \n       VEHICLE_TYPE SPEEDING_VIOLATIONS                DUIS      PAST_ACCIDENTS \n                  0                   0                   0                   0 \n            OUTCOME \n                  0 \n\n\n\n\n\nAnother necessary step to consider, especially when modeling probabilities is Outliers. Which has the ability to misrepresent the relationship between predictors and the target variable. If extreme values in predictors like ANNUAL_MILEAGE or PAST_ACCIDENTS exist, they might disproportionately influence the model’s weights, leading to biased odds ratios by inflating coefficients and producing misleading conclusions. From a business perspective, consider an extreme case like 22 speeding violations in the SPEEDING_VIOLATIONS variable. Cases like 22 speeding violations may indicate data errors or fraud, which would skew risk assessments.\n\n\nCode\nnumeric_var = c(\"PAST_ACCIDENTS\", \"SPEEDING_VIOLATIONS\", \"DUIS\", \"ANNUAL_MILEAGE\")  \n\n# Density distributions for all relevant variables\ndistribution_fig = ggplot(data_car %&gt;% pivot_longer(all_of(numeric_var)), aes(value, fill = name)) +  \n  geom_density(alpha = 0.6) +  \n  facet_wrap(~name, scales = \"free\", ncol = 2) +  \n  labs(  \n    title = \"Distribution of Key Variables (Before Capping)\",  \n    subtitle = \"Right-skewed distributions with extreme values (e.g., 22 speeding violations)\"  \n  ) +  \n  theme_classic() +  \n  theme(legend.position = \"none\") \n\ndistribution_fig \n\n\n\n\n\nBased on the distributions shown in the plot, capping DUIS, PAST_ACCIDENTS, and SPEEDING_VIOLATIONS appears reasonable, as these variables showcases a strong right skew with extreme values. The ANNUAL_MILEAGE variable, on the other hand, appears approximately normally distributed, making capping unnecessary.\nTo alleviate the impact of Outliers on the model’s coefficient estimates, we use a capping method. Capping helps to avoid biases while preserving data size and statistical power, which is important in a time-constrained project as ours. The variables targeted for capping are of a numeric nature. Mainly DUIS, PAST_ACCIDENTS, and SPEEDING_VIOLATIONS distribution, which follows a exponential distribution (see distribution figure), we would use a capping to reduce the exponential effect on our model. Variables in questions are PAST_ACCIDENTS,SPEEDING_VIOLATIONS, DUIS.\n\n\nCode\n# Capping function with thresholds \ncap_custom = function(x, variable_n) { \n  \n  threshold = case_when(  \n    variable_n == \"DUIS\" ~ 3,  \n    variable_n == \"PAST_ACCIDENTS\" ~ 4,  \n    variable_n == \"SPEEDING_VIOLATIONS\" ~ 6,  \n    TRUE ~ max(x, na.rm = TRUE)  # while no cap for the other variables  \n  )  \n  pmin(x, threshold)  \n}  \n\n# Then applying it on the dataset\ndata_car = data_car %&gt;%  \n  mutate(  \n    DUIS = cap_custom(DUIS, \"DUIS\"),  \n    PAST_ACCIDENTS = cap_custom(PAST_ACCIDENTS, \"PAST_ACCIDENTS\"),  \n    SPEEDING_VIOLATIONS = cap_custom(SPEEDING_VIOLATIONS, \"SPEEDING_VIOLATIONS\")  \n  )  \n\n# Distribution figure after capping\nggplot(data_car %&gt;%  \n         pivot_longer(c(DUIS, PAST_ACCIDENTS, SPEEDING_VIOLATIONS)),  \n       aes(value, fill = name)) +  \n  geom_density(alpha = 0.6) +  \n  facet_wrap(~name, scales = \"free\", ncol = 3) +  \n  labs(  \n    title = \"Distributions After Threshold Capping\",  \n    subtitle = \"DUIS ≤2 | Accidents ≤4 | Speeding Violations ≤6\"  \n  ) +  \n  theme_classic() +  \n  theme(legend.position = \"none\")  \n\n\n\n\n\nBased on the figure above, the thresholds were strategically determined based on distribution analysis (see figure 1). Specifically, a threshold of 2 was implemented for DUIS. This balances the need to capture drivers with multiple offenses while limiting the impact of outlier values on the model. Given the observed distribution of PAST_ACCIDENTS, a threshold of 4 was chosen, acknowledging that most drivers have relatively few prior accidents. Lastly, for SPEEDING_VIOLATIONS, a threshold of 6 was selected, with intention to reflect a balance between capturing risky driving behavior and reducing the effects of extreme, potentially extreme values.\n\n\n\nBefore building the model, we need to refine the dataset further, starting with addressing the class imbalance in the target variable (OUTCOME). A common issue with class imbalances, particularly when it involves the target variable, is that most machine learning models tend to prioritize predicting the majority class. In this case, the majority class is non-claims as the image below shows, and models often focus on it to maximize overall accuracy. Meaning that with uneven class distribution, the model will “cheat” by showing biasness toward majority class (no claims) to maximize its accuracy while ignoring the minority, from a business perspective, it would mean ignoring high risk drivers.\nStatistically, this can lead to misclassifying high-risk drivers as low-risk (false negatives, FN), which could result in financial losses for the insurer. We will address this using techniques like class weighting (Weighted GLM) to ensure the model does not overlook critical risk patterns.\nFor example, a driver who is 34 years old, drives a sports car, and has had four past accidents would be considered high-risk. If such a driver is classified as low-risk by the model, it could lead to significant financial losses for the insurer due to missed risk assessments. Without corrections to address the imbalance, such cases could be missed frequently and insurer will be approving risky policies due to biased model.\n\n\n\n\nCode\n# Plot class distribution\nclass_dist = ggplot(data_car, aes(x = factor(OUTCOME), fill = factor(OUTCOME))) +\n  geom_bar() +\n  scale_fill_manual(values = c(\"blue\", \"red\")) +\n  labs(\n    title = \"Class Distribution of Insurance Claims\",\n    x = \"Claim Status (0 = No Claim, 1 = Claim)\",\n    y = \"Count\",\n    fill = \"Outcome\"\n  ) +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\nclass_dist\n\n\n\n\n\nThe plot above addresses the class imbalance. It appears that the majority class has over twice as much observations as the minority class, to be precis a class ratio: 68.7% (no claim) vs. 31.3% (claim), a 2.2:1 imbalance. By adding weights to the minority class the model will reduce incidents of false negative (risky drivers being approved) but mostly likely increase false positives (safe drivers being rejected, FP) with the main priority here from insurer point of view is to minimize financial losses over lost revenue due to rejecting safe drivers. the selection of the optimal weights will be based on the F-score, a metric often used to balance between precision and recall, and in our case maximizing the F-score helps ensure we accurately identify high-risk drivers without overly flagging low-risk ones.\n\n\nCode\n# we define and test diff weights,  1.5 - 2.5\nweights_to_test = seq(1.5, 2.5, by = 0.1)\n\n# Store\nresults = data.frame(weight = weights_to_test, f_score = numeric(length(weights_to_test)))\n\n# Loop and then evaluate F-scores\nfor (i in seq_along(weights_to_test)) {\n  model = glm(\n    OUTCOME ~ .,\n    data = data_car,\n    family = \"binomial\",\n    weights = ifelse(OUTCOME == 1, weights_to_test[i], 1)\n  )\n  predictions = ifelse(predict(model, type = \"response\") &gt; 0.5, 1, 0)\n  cm = confusionMatrix(factor(predictions, levels = c(0, 1)), factor(data_car$OUTCOME, levels = c(0, 1)))\n  results$f_score[i] = cm$byClass[\"F1\"]  \n}\n\n# Selecting the optimal weight with maximum F-score\noptimal_weight = results$weight[which.max(results$f_score)]\noptimal_f1 = max(results$f_score)\n\n\n# Plot results with highlighted optimal weight\nggplot(results, aes(weight, f_score)) +\n  geom_line(color = \"black\") +\n  geom_point(color = \"orange\") +\n  geom_point(data = results[which.max(results$f_score), ], aes(weight, f_score),\n             color = \"red\", size = 4) + \n  labs(\n    title = \"Optimal Weight for Class Balancing\",\n    x = \"Weight for Claims (OUTCOME=1)\",\n    y = \"F-score\"\n  ) +\n  theme_classic()\n\n\n\n\n\nNow, we have obtained the optimal weight based on the F-score, which will be used when we train the model.\n\n\n\nnow, to ensure transparency, we will be using all the predictors in the logistic model. A step to align with best practices for interpretable insurance risk modeling, where domain knowledge and regulatory compliance prioritize comprehensive risk assessment over algorithmic simplicity.\nOur rationale includes:\n\nRegulatory Requirements: Solvency II mandates insurers to justify all risk drivers, even marginally significant ones.\nField knowledge: Different variables like EDUCATION and RACE may directly or indirectly influence risk, but at the same time provides socioeconomic context.\nResearch alignment: Hsu et al. (2016b) emphasize interaction terms to capture compounding risk effects.\n\nIn this step, we are creating a couple of new variables with a clear intention to capture the synergistic effects between variables, meaning the combined impact of two predictors. Which could be considered greater than their individual effects.This addresses the multiplicative nature of insurance risk (Goldburd et al., 2020), where risks is rarely additive rather they tend to be additive."
  },
  {
    "objectID": "projects/predictive_risk_modeling.html#model-specification",
    "href": "projects/predictive_risk_modeling.html#model-specification",
    "title": "Predictive Risk Modeling for Auto Insurance: Claim Likelihood Estimation",
    "section": "",
    "text": "To capture the synergistic effects between variables critical to our model, we first structure the data to enable such complex interactions when creating interaction terms during the training phase. This process involves two sequential steps:\nFirst, we encode categorical variables (e.g., driver age, vehicle type) as factors. This ensures the model treats categories as distinct risk tiers (e.g., “AGE 16-25” vs. “65+”) rather than numeric values, to maintain relationships critical for interaction terms. A step which align with Goldburd et al. (2020), who emphasize that insurance risks interact multiplicatively and not additively, which justify this approach.\nNext, we split the data into training (70%) and test (30%) sets. This division is critical to evaluate model performance on unseen data, but also to ensure that the identified interactions generalize beyond the training environment.\n\n\nCode\n# Turning categorical variables to factors \ndata_car = data_car %&gt;%\n  mutate(\n    AGE = factor(AGE, levels = c(\"16-25\", \"26-39\", \"40-64\", \"65+\")),\n    GENDER = factor(GENDER, levels = c(\"female\", \"male\")),\n    RACE = factor(RACE, levels = c(\"majority\", \"minority\")),\n    \n    DRIVING_EXPERIENCE = factor(DRIVING_EXPERIENCE, \n                                levels = c(\"0-9y\", \"10-19y\", \"20-29y\", \"30y+\")),\n    \n    EDUCATION = factor(EDUCATION, levels = c(\"none\", \"high school\", \"university\")),\n    INCOME = factor(INCOME, levels = c(\"poverty\", \"working class\", \"middle class\", \"upper class\")),\n    \n    VEHICLE_YEAR = factor(VEHICLE_YEAR, levels = c(\"before 2015\", \"after 2015\")),\n    VEHICLE_TYPE = factor(VEHICLE_TYPE, levels = c(\"sedan\", \"sports car\")),\n    \n    MARRIED = factor(MARRIED, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    CHILDREN = factor(CHILDREN, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    VEHICLE_OWNERSHIP = factor(VEHICLE_OWNERSHIP, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n        OUTCOME = factor(OUTCOME, levels = c(0, 1))\n)\n\ndata_car$ANNUAL_MILEAGE = data_car$ANNUAL_MILEAGE / 1000\n\n# data shuffling and splitting 70/30 \nset.seed(987)\n\nshu_data = data_car[sample(nrow(data_car)), ]\n\ntrain_data = shu_data[1:7000, ]\ntest_data = shu_data[7001:10000, ]\n\n\n\n\nThis section presents our model applied to the training dataset, together with key interaction terms. The model quantifies how demographic, behavioral, and vehicle-related factors influence claim likelihood, with a special focus on the interactions “AGE:VEHICLE_TYPE” and “PAST_ACCIDENTS:SPEEDING_VIOLATIONS.\nFor easier interpretation, we transform the estimated coefficients into odds ratios and visualize these ratios. The plot below categorizes predictors as risk drivers (red) or risk reducers (green), using asterisks (*) to denote statistical significance. This approach could be useful for insurers to identify opportunities to adjust premiums for high-risk groups or to reward safer behaviors.\n\n\nCode\n# Final Model (Training Data)\nfinal_model = glm(\n  OUTCOME ~ AGE + GENDER + RACE + DRIVING_EXPERIENCE + EDUCATION + INCOME +\n    CREDIT_SCORE + VEHICLE_OWNERSHIP + VEHICLE_YEAR + MARRIED + CHILDREN +\n    ANNUAL_MILEAGE + VEHICLE_TYPE + SPEEDING_VIOLATIONS + DUIS + PAST_ACCIDENTS +\n    AGE:VEHICLE_TYPE + PAST_ACCIDENTS:SPEEDING_VIOLATIONS,\n  data = train_data,\n  family = \"binomial\",\n  weights = ifelse(OUTCOME == 1, optimal_weight, 1)\n)\n\n# summary\nmodelsummary(final_model, output = \"markdown\", stars = T)\n\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                (1)\n              \n        \n        + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n        \n                \n                  (Intercept)                         \n                  1.612*** \n                \n                \n                                                      \n                  (0.256)  \n                \n                \n                  AGE26-39                            \n                  -0.223*  \n                \n                \n                                                      \n                  (0.109)  \n                \n                \n                  AGE40-64                            \n                  -0.107   \n                \n                \n                                                      \n                  (0.127)  \n                \n                \n                  AGE65+                              \n                  -0.164   \n                \n                \n                                                      \n                  (0.160)  \n                \n                \n                  GENDERmale                          \n                  0.974*** \n                \n                \n                                                      \n                  (0.068)  \n                \n                \n                  RACEminority                        \n                  -0.264*  \n                \n                \n                                                      \n                  (0.106)  \n                \n                \n                  DRIVING_EXPERIENCE10-19y            \n                  -1.578***\n                \n                \n                                                      \n                  (0.112)  \n                \n                \n                  DRIVING_EXPERIENCE20-29y            \n                  -3.208***\n                \n                \n                                                      \n                  (0.179)  \n                \n                \n                  DRIVING_EXPERIENCE30y+              \n                  -3.636***\n                \n                \n                                                      \n                  (0.287)  \n                \n                \n                  EDUCATIONhigh school                \n                  -0.029   \n                \n                \n                                                      \n                  (0.088)  \n                \n                \n                  EDUCATIONuniversity                 \n                  -0.179+  \n                \n                \n                                                      \n                  (0.105)  \n                \n                \n                  INCOMEworking class                 \n                  0.089    \n                \n                \n                                                      \n                  (0.103)  \n                \n                \n                  INCOMEmiddle class                  \n                  -0.015   \n                \n                \n                                                      \n                  (0.124)  \n                \n                \n                  INCOMEupper class                   \n                  0.008    \n                \n                \n                                                      \n                  (0.156)  \n                \n                \n                  CREDIT_SCORE                        \n                  -0.145   \n                \n                \n                                                      \n                  (0.343)  \n                \n                \n                  VEHICLE_OWNERSHIPYes                \n                  -1.599***\n                \n                \n                                                      \n                  (0.072)  \n                \n                \n                  VEHICLE_YEARafter 2015              \n                  -1.695***\n                \n                \n                                                      \n                  (0.085)  \n                \n                \n                  MARRIEDYes                          \n                  -0.312***\n                \n                \n                                                      \n                  (0.074)  \n                \n                \n                  CHILDRENYes                         \n                  -0.163*  \n                \n                \n                                                      \n                  (0.073)  \n                \n                \n                  ANNUAL_MILEAGE                      \n                  0.060*** \n                \n                \n                                                      \n                  (0.014)  \n                \n                \n                  VEHICLE_TYPEsports car              \n                  -0.323   \n                \n                \n                                                      \n                  (0.287)  \n                \n                \n                  SPEEDING_VIOLATIONS                 \n                  0.073+   \n                \n                \n                                                      \n                  (0.039)  \n                \n                \n                  DUIS                                \n                  0.035    \n                \n                \n                                                      \n                  (0.077)  \n                \n                \n                  PAST_ACCIDENTS                      \n                  -0.312***\n                \n                \n                                                      \n                  (0.057)  \n                \n                \n                  AGE26-39 × VEHICLE_TYPEsports car   \n                  0.559    \n                \n                \n                                                      \n                  (0.356)  \n                \n                \n                  AGE40-64 × VEHICLE_TYPEsports car   \n                  0.831*   \n                \n                \n                                                      \n                  (0.390)  \n                \n                \n                  AGE65+ × VEHICLE_TYPEsports car     \n                  1.500**  \n                \n                \n                                                      \n                  (0.522)  \n                \n                \n                  SPEEDING_VIOLATIONS × PAST_ACCIDENTS\n                  0.009    \n                \n                \n                                                      \n                  (0.020)  \n                \n                \n                  Num.Obs.                            \n                  7000     \n                \n                \n                  AIC                                 \n                  7558.4   \n                \n                \n                  BIC                                 \n                  7750.3   \n                \n                \n                  Log.Lik.                            \n                  -3751.203\n                \n                \n                  F                                   \n                  77.684   \n                \n                \n                  RMSE                                \n                  0.34     \n                \n        \n      \n    \n\n\n\nCode\n# plot Risk driver vs. risk reducer predictors. Odds \nmodel_summary = tidy(final_model)\nodds_table = data.frame(\n  Predictor = names(exp(coef(final_model))),\n  Odds_Ratio = round(exp(coef(final_model)), 2),\n  p_value = model_summary$p.value\n) %&gt;%\n  filter(Predictor != \"(Intercept)\") %&gt;%\n  mutate(\n    Effect_Type = ifelse(Odds_Ratio &gt; 1, \"Risk Driver\", \"Risk Reducer\"),\n    Predictor_Label = ifelse(\n      p_value &lt; 0.1,\n      paste0(\"&lt;b&gt;\", Predictor, \"&lt;/b&gt;\"),  # Bold text for p &lt; 0.1\n      Predictor\n    ),\n    Odds_Label = case_when(\n      p_value &lt; 0.01 ~ paste0(Odds_Ratio, \"***\"),\n      p_value &lt; 0.05 ~ paste0(Odds_Ratio, \"**\"),\n      p_value &lt; 0.1 ~ paste0(Odds_Ratio, \"*\"),\n      TRUE ~ as.character(Odds_Ratio)\n    )\n  ) %&gt;%\n  arrange(desc(Odds_Ratio))\n\nodds_plot = ggplot(odds_table, aes(x = reorder(Predictor_Label, Odds_Ratio), y = Odds_Ratio, fill = Effect_Type)) +\n  geom_col(alpha = 0.8) +\n  geom_hline(yintercept = 1, linetype = \"dashed\", color = \"gray40\") +\n  geom_text(aes(label = Odds_Label), hjust = -0.2, size = 3.5) +\n  coord_flip() +\n  scale_fill_manual(values = c(\"Risk Driver\" = \"#E74C3C\", \"Risk Reducer\" = \"#2ECC71\")) +\n  labs(\n    title = \"All Predictors: Significant Risk Drivers/Reducers \",\n    subtitle = \"Odds Ratio &gt; 1 = Higher Risk | * = p &lt; 0.1, ** = p &lt; 0.05, *** = p &lt; 0.01\",\n    x = \"Predictor\",\n    y = \"Odds Ratio (Reference = 1)\",\n    fill = \"Effect Type\"\n  ) +\n  theme_classic() +\n  theme(\n    legend.position = \"bottom\",\n    axis.text.y = element_markdown(size = 10, color = \"black\"),\n    plot.margin = margin(20, 100, 20, 20)\n  )\nodds_plot\n\n\n\n\n\nResults show significant variation in predictors’ statistical power. Odds ratios serve as claim likelihood multipliers, where a value of one indicates no effect relative to the reference group, values above one suggest increased odds, and values below one indicate reduced odds. For example, an odds ratio of two means double the claim odds compared to the reference group. All interpretations are relative to baseline categories (e.g., female for gender, youngest age group, least experienced drivers).\nObserving the plot, there’s three main risk drivers that emerge: Individuals aged 65+ driving sports cars exhibit 4.48 times higher claim odds than the reference group, representing the strongest risk driver. Male drivers show 2.65 times higher odds than females. The 40-64 age group driving sports cars demonstrates 2.3 times higher claim odds. These results are consistent with industry expectations, as both sports cars and certain demographic traits are well-known to increase risk, and a combination of these factors clearly amplifies the effect.\nOn the other hand, the main risk reducers include 30+ years of driving experience (97% lower claims), 20-29 years of experience (96% lower claims), and vehicles produced after 2015 (82% lower claims). It appears that possessing driving experience and newer vehicle has a protective effect on filing a claim. Remarkably, past accidents correlate with 27% lower claims, suggesting behavioral changes post-accident, while marriage similarly reduces odds by 27%.\nOne notable and somewhat surprising result is that past accidents is correlate with 27% lower claims, indicating a post-accident behavioral shift. The same is observable in regard to marriage, as it correlate with 27% lower claim. Overall, we can say that the model performs as expected, as these findings provide clear guidance for targeted premium adjustments, ensuring that pricing reflects the actual risk profile revealed by the data.\n\n\n\nPreviously, we explained how each predictor affects the odds of an insurance claim, showing whether a factor increases or decreases risk. Now, we focus on a different but equally important question: Which variables have the biggest impact on the model’s predictions, and by how much do they influence the outcome for each case? This is where SHAP values are valuable. SHAP values measure how much each predictor contributes, on average, to increasing or decreasing the model’s output for individual cases. A higher mean SHAP value means that feature has a greater overall impact on the model’s predictions, regardless of whether it increases or decreases the risk. Giving us a clearer and more practical understanding of how the model makes its prediction.\n\n\nCode\nexplainer = DALEX::explain(\n  model = final_model,\n  data  = as.data.frame(train_data)[ , setdiff(names(train_data), \"OUTCOME\") ],\n  # Convert factor to 0/1 on the fly:\n  y     = as.numeric(train_data$OUTCOME) - 1,\n  # Default predict_function will return log-odds; we want probabilities:\n  predict_function = function(m, newdata) {\n    predict(m, newdata = newdata, type = \"response\")\n  },\n  label = \"Logistic Model\"\n)\n\n\nPreparation of a new explainer is initiated\n  -&gt; model label       :  Logistic Model \n  -&gt; data              :  7000  rows  16  cols \n  -&gt; target variable   :  7000  values \n  -&gt; predict function  :  function(m, newdata) {     predict(m, newdata = newdata, type = \"response\") } \n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package stats , ver. 4.2.2 , task classification (  default  ) \n  -&gt; predicted values  :  numerical, min =  0.001020305 , mean =  0.358588 , max =  0.9759593  \n  -&gt; residual function :  difference between y and yhat (  default  )\n  -&gt; residuals         :  numerical, min =  -0.9713287 , mean =  -0.0483023 , max =  0.9944156  \n  A new explainer has been created!  \n\n\nCode\nshap_values = predict_parts(\n  explainer,\n  new_observation = train_data[1:100, ],\n  type            = \"shap\",\n  B               = 100\n)\n\nshap_aggregated = shap_values %&gt;%\n  group_by(variable) %&gt;%\n  summarize(mean_abs_shap = mean(abs(contribution)), .groups = \"drop\") %&gt;%\n  arrange(desc(mean_abs_shap))\n\n# \nggplot(shap_aggregated, aes(\n    x = reorder(variable, mean_abs_shap),\n    y = mean_abs_shap\n  )) +\n  geom_col(fill = \"#2F4F4F\", width = 0.7) +\n  coord_flip() +\n  labs(\n    title    = \"Global Feature Importance: SHAP Values\",\n    subtitle = \"Top Drivers of Insurance Claim Predictions\",\n    x        = \"Predictors\",\n    y        = \"Mean |SHAP Contribution|\"\n  ) +\n  theme_classic() +\n  theme(\n    plot.title    = element_text(face = \"bold\", size = 14),\n    axis.text.y   = element_text(size = 10)\n  )\n\n\n\n\n\nBased on the plot, the most influential variable by far is having 20 to 29 years of driving experience. This predictor stands out with a much higher SHAP value than any other, suggesting that, in this dataset, drivers in this experience group consistently have a strong effect, either increasing or decreasing the model’s predicted probability of a claim. In fact, as shown in the previous odds ratio plot, this predictor is considered a risk reducer.\nThe next most important features are vehicle ownership status and gender, specifically being female. Both of these variables have notable contributions to the model’s predictions, suggesting that whether someone owns their vehicle and their gender play significant roles in how the model assesses risk. Another important observation includes driving a car manufactured before 2015 and being married. Similarly, as we saw in the odds ratio plot, driving a car produced after 2015 appeared to reduce the odds of filing a claim relative to the reference group, which is cars produced before 2015. Following this logic, driving a car before 2015 can be considered a risk contributor. Being married also appears meaningful in the SHAP plot, as it was in the odds ratio plot, though to a lesser extent.\nSeveral other predictors, such as the number of speeding violations, past accidents, education level, and whether the driver has children, have smaller but still measurable impacts on the model’s predictions. Intriguingly, factors like annual mileage, past accidents, driving experience above 30 years, and having children appear to have relatively little to no effect according to the SHAP analysis, even though some of these might be expected to have a noticeable influence.\nThis simply implies that, for this particular model and dataset, experience, car ownership, and certain demographic factors are the dominant drivers of predicted claim risk. Overall, it is plausible to say that these insights can be helpful to raise attention on the characteristics that influence claim risk, especially when we consider the fact that these insights still lie within the traditional assumptions in the insurance industry and can support more focused risk management and pricing strategies ### 3.3 Model Performance and metrics\n\n\nCode\n# Generate predictions on test data\ntest_pred_prob = predict(final_model, newdata = test_data, type = \"response\")\ntest_pred_class = ifelse(test_pred_prob &gt; 0.5, 1, 0)  # Threshold = 0.5\n\n# ROC-AUC\nroc_curve = roc(test_data$OUTCOME, test_pred_prob)\n\n\nSetting levels: control = 0, case = 1\n\n\nSetting direction: controls &lt; cases\n\n\nCode\nauc_value = auc(roc_curve)\nroc_curve\n\n\n\nCall:\nroc.default(response = test_data$OUTCOME, predictor = test_pred_prob)\n\nData: test_pred_prob in 2039 controls (test_data$OUTCOME 0) &lt; 961 cases (test_data$OUTCOME 1).\nArea under the curve: 0.9008\n\n\nCode\n# Plot ROC Curve\nplot(roc_curve, main = \"ROC Curve (Test Data)\", col = \"#2F4F4F\", print.auc = TRUE)\n\n\n\n\n\n\n\nCode\n# Generate confusion matrix\nconf_matrix = confusionMatrix(\n  factor(test_pred_class, levels = c(0, 1)),\n  factor(test_data$OUTCOME, levels = c(0, 1)),\n  positive = \"1\"\n)\n# Convert to dataframe\ncm_df = as.data.frame(conf_matrix$table)\n\n# Create label for each cell\ncm_df = cm_df %&gt;%\n  mutate(Label = case_when(\n    Prediction == 1 & Reference == 1 ~ \"True Positive\",\n    Prediction == 0 & Reference == 1 ~ \"False Negative\",\n    Prediction == 1 & Reference == 0 ~ \"False Positive\",\n    Prediction == 0 & Reference == 0 ~ \"True Negative\"\n  ))\n\n# Plot \nggplot(cm_df, aes(x = Reference, y = Prediction, fill = Freq)) +\n  geom_tile(color = \"white\") +\n  geom_text(aes(label = paste0(Label, \"\\n\", Freq)), color = \"white\", size = 5) +\n  scale_fill_gradient(low = \"#2F4F4F\", high = \"#E74C3C\") +\n  labs(\n    title = \"Confusion Matrix with Labelled Outcomes (Threshold = 0.5)\",\n    x = \"Actual Outcome\",\n    y = \"Predicted Outcome\") +\n  theme_classic()\n\n\n\n\n\nlets see how well the model ranks clients using AUC, a metric mostly used to determine the classification power of the model to differentiate between high vs. low clients. while using confusion metric will help us Show the exact counts of misclassifications (false negatives/positives) which could be helpful in the next step, where we intend to apply it in a practical business related scenarios with simulations."
  },
  {
    "objectID": "projects/predictive_risk_modeling.html#business-impact-with-interactive-dashboard",
    "href": "projects/predictive_risk_modeling.html#business-impact-with-interactive-dashboard",
    "title": "Predictive Risk Modeling for Auto Insurance: Claim Likelihood Estimation",
    "section": "",
    "text": "With this interactive dashboard, users can explore how different risk thresholds and cost settings shape the insurer’s financial outcomes. The left panel allows users to set and adjust key assumptions, such as the risk threshold, costs of false negatives and false positives, and average premium and average claim. Any changes update the results instantly, showing the direct impact on net savings and other important model metrics. On the right, the dashboard provides a clear visual summary of financial trade-offs between risk capture (true positives) and misclassifications costs. Making it easy for both technical and non-technical audience to identify the most cost-effective risk management strategy for the insurance portfolio.\n\nCost-benefits visualization: The 3D sensitivity plot explores how the cost per false negative (FN), cost per false positive (FP), and net savings interact. Supplying us with a clear view of costs and benefits under different scenarios.\nInstant Recommendations: With a single click, the “Optimize Threshold” button finds the optimal risk threshold that maximizes net savings for your chosen input values. While the “Calculate Optimal Premium” button suggests the most profitable premium to charge for the given scenario.\nLive Key Metrics: The dynamic metrics table updates automatically as you change inputs, displaying essential model statistics like accuracy, precision, and cost breakdowns. This helps the users to assess how reliable and effective our model are under different settings.\n\nOverall, our goal with this dashboard is not only to make complex risk and cost calculations transparent but also to showcase how regression models, like logistic regression, can be powerful tools for optimizing net savings in risk analysis.\n\n\nCode\n# Preparing data\ntest_data$OUTCOME = factor(test_data$OUTCOME, levels = c(\"0\", \"1\"))\ntest_pred_prob = predict(final_model, newdata = test_data, type = \"response\")\nroc_curve = roc(test_data$OUTCOME, test_pred_prob)\n\n# Precompute actual claims count\nactual_claims = sum(test_data$OUTCOME == \"1\")\n\n# UI Definition\nui = dashboardPage(\n  dashboardHeader(title = \"Risk Model Dashboard\"),\n  dashboardSidebar(\n    sliderInput(\"threshold\", \"Risk Threshold:\", 0, 1, 0.5, step = 0.01),\n    numericInput(\"cost_fn\", \"Cost per False Negative (SEK):\", 7000),\n    numericInput(\"cost_fp\", \"Cost per False Positive (SEK):\", 4000),\n    numericInput(\"premium\", \"Average Premium (SEK):\", value = 3000),\n    numericInput(\"claim\", \"Average Claim (SEK):\", value = 12000),\n    actionButton(\"optimize\", \"Find Optimal Threshold\"),\n    actionButton(\"optimize_premium\", \"Calculate Optimal Premium\")\n  ),\n  dashboardBody(\n    tabBox(\n      width = 12,\n      tabPanel(\n        \"Performance\",\n        fluidRow(\n          box(plotlyOutput(\"roc_curve\"), width = 6),\n          box(plotOutput(\"conf_matrix\"), width = 6)\n        )\n      ),\n      tabPanel(\n        \"Financial Impact\",\n        fluidRow(\n          column(6, box(plotlyOutput(\"savings_breakdown\"), width = 12, \n                        title = \"Financial Impact Breakdown\")),\n          column(6, box(DTOutput(\"metrics_table\"), width = 12, \n                        title = \"Key Metrics\", \n                        style = \"overflow-y: auto; height: 400px;\"))\n        ),\n        fluidRow(\n          box(plotlyOutput(\"sensitivity_3d\", height = \"500px\"), width = 6,\n              title = \"3D Cost Sensitivity Analysis\"),\n          box(plotlyOutput(\"savings_sensitivity\"), width = 6,\n              title = \"2D Cost Sensitivity\", height = \"500px\")\n        )\n      )\n    )\n  )\n)\n\nserver = function(input, output, session) {\n  # Precomputed actual claims\n  actual_claims_val = reactiveVal(sum(test_data$OUTCOME == \"1\"))\n  \n  # Reactive confusion matrix\n  conf_matrix = reactive({\n    req(test_pred_prob, test_data$OUTCOME)\n    pred_class = ifelse(test_pred_prob &gt; input$threshold, 1, 0)\n    confusionMatrix(factor(pred_class), test_data$OUTCOME, positive = \"1\")\n  })\n  \n  # FINANCIAL CALCULATIONS (REVISED)\n  financials = reactive({\n    cm = conf_matrix()$table\n    TP = cm[\"1\",\"1\"]   # Correctly rejected high-risk\n    TN = cm[\"0\",\"0\"]   # Correctly accepted low-risk\n    FP = cm[\"1\",\"0\"]   # Wrongly rejected safe clients\n    FN = cm[\"0\",\"1\"]   # Wrongly accepted high-risk\n    \n    # Core financial metrics\n    Revenue = (TN + FN) * input$premium  # Premiums from accepted policies\n    Claim_Payouts = FN * input$claim     # Claims paid for accepted high-risk\n    \n    # Cost components\n    FP_Cost = FP * input$cost_fp         # Opportunity cost of rejecting safe clients\n    Operational_Costs = 0.15 * (FP_Cost + Claim_Payouts)  # Scaling with workload\n    \n    # Profit calculation\n    Profit = Revenue - Claim_Payouts - FP_Cost - Operational_Costs\n    \n    # No-model baseline (accept all applicants)\n    Baseline_Profit = (nrow(test_data) * input$premium) - (actual_claims_val() * input$claim)\n    \n    # Net savings vs. baseline\n    Net_Savings = Profit - Baseline_Profit\n    \n    list(\n      TP = TP, FN = FN, FP = FP, TN = TN,\n      Revenue = Revenue,\n      Claim_Payouts = Claim_Payouts,\n      FP_Cost = FP_Cost,\n      Operational_Costs = Operational_Costs,\n      Profit = Profit,\n      Baseline_Profit = Baseline_Profit,\n      Net_Savings = Net_Savings\n    )\n  })\n  \n  # 3D Sensitivity Analysis\n  output$sensitivity_3d = renderPlotly({\n    req(test_pred_prob, test_data$OUTCOME)\n    \n    fn_range = seq(input$cost_fn * 0.5, input$cost_fn * 2, length.out = 10)\n    fp_range = seq(input$cost_fp * 0.5, input$cost_fp * 2, length.out = 10)\n    thresholds = seq(0.1, 0.9, length.out = 10)\n    \n    grid = expand.grid(\n      fn_cost = fn_range,\n      fp_cost = fp_range,\n      threshold = thresholds\n    )\n    \n    grid$net_savings = mapply(function(fn, fp, t) {\n      pred_class = ifelse(test_pred_prob &gt; t, 1, 0)\n      cm = confusionMatrix(factor(pred_class), test_data$OUTCOME, positive = \"1\")$table\n      TP = cm[\"1\",\"1\"]\n      FN = cm[\"0\",\"1\"]\n      TN = cm[\"0\",\"0\"]\n      FP = cm[\"1\",\"0\"]\n      \n      # Revised profit calculation\n      Revenue = (TN + FN) * input$premium\n      Claim_Payouts = FN * input$claim\n      FP_Cost = FP * fp\n      Operational_Costs = 0.15 * (FP_Cost + Claim_Payouts)\n      \n      Profit = Revenue - Claim_Payouts - FP_Cost - Operational_Costs\n      Baseline_Profit = (nrow(test_data) * input$premium) - (actual_claims_val() * input$claim)\n      \n      Profit - Baseline_Profit  # Net savings\n    }, grid$fn_cost, grid$fp_cost, grid$threshold)\n    \n    plot_ly(grid, x = ~fn_cost, y = ~fp_cost, z = ~net_savings,\n            color = ~threshold, colors = viridis::viridis(100),\n            type = \"scatter3d\", mode = \"markers\",\n            marker = list(size = 5)) %&gt;%\n      layout(\n        scene = list(\n          xaxis = list(title = \"FN Cost (SEK)\"),\n          yaxis = list(title = \"FP Cost (SEK)\"),\n          zaxis = list(title = \"Net Savings (SEK)\"),\n          camera = list(eye = list(x = 1.5, y = -1.5, z = 0.5))\n        ),\n        title = list(text = \"3D Cost Sensitivity Analysis\", y = 0.98)\n      )\n  })\n  \n  # Inside server function - replace the existing observeEvent for threshold optimization\nobserveEvent(input$optimize, {\n  showNotification(\"Calculating optimal threshold...\", duration = 3)\n  \n  thresholds = seq(0.1, 0.9, by = 0.01)\n  savings = sapply(thresholds, function(t) {\n    pred_class = ifelse(test_pred_prob &gt; t, 1, 0)\n    cm = confusionMatrix(factor(pred_class), test_data$OUTCOME, positive = \"1\")$table\n    TP = cm[\"1\",\"1\"]\n    FN = cm[\"0\",\"1\"]\n    TN = cm[\"0\",\"0\"]\n    FP = cm[\"1\",\"0\"]\n    \n    Revenue = (TN + FN) * input$premium\n    Claim_Payouts = FN * input$claim\n    FP_Cost = FP * input$cost_fp\n    Operational_Costs = 0.15 * (FP_Cost + Claim_Payouts)\n    \n    Profit = Revenue - Claim_Payouts - FP_Cost - Operational_Costs\n    Baseline_Profit = (nrow(test_data) * input$premium) - (actual_claims_val() * input$claim)\n    \n    Profit - Baseline_Profit  # Net savings\n  })\n  \n  optimal = thresholds[which.max(savings)]\n  updateSliderInput(session, \"threshold\", value = optimal)\n})\n\n  \n  # Optimal Premium Calculation\n  observeEvent(input$optimize_premium, {\n    cm = conf_matrix()$table\n    FN = cm[\"0\",\"1\"]\n    TN = cm[\"0\",\"0\"]\n    \n    # Risk-adjusted premium calculation\n    Expected_Claims = FN / (TN + FN)  # Claim rate among accepted\n    Risk_Load = 0.25  # 25% risk loading\n    optimal_premium = input$claim * Expected_Claims * (1 + Risk_Load)\n    \n    updateNumericInput(session, \"premium\", value = round(optimal_premium, -2))\n  })\n  \n  # FIXED: 2D Sensitivity Plot\n  output$savings_sensitivity = renderPlotly({\n    req(input$cost_fn, input$cost_fp, input$premium, input$claim)\n    \n    cost_fn_range = seq(input$cost_fn * 0.5, input$cost_fn * 1.5, length.out = 20)\n    cost_fp_range = seq(input$cost_fp * 0.5, input$cost_fp * 1.5, length.out = 20)\n    \n    grid = expand.grid(cost_fn = cost_fn_range, cost_fp = cost_fp_range)\n    \n    # Calculate net savings for each combination\n    grid$net_savings = mapply(function(fn, fp) {\n      # Get current confusion matrix\n      cm = conf_matrix()$table\n      TP = cm[\"1\",\"1\"]\n      FN = cm[\"0\",\"1\"]\n      TN = cm[\"0\",\"0\"]\n      FP = cm[\"1\",\"0\"]\n      \n      # Calculate financials with current parameters\n      Revenue = (TN + FN) * input$premium\n      Claim_Payouts = FN * input$claim\n      FP_Cost = FP * fp\n      Operational_Costs = 0.15 * (FP_Cost + Claim_Payouts)\n      \n      Profit = Revenue - Claim_Payouts - FP_Cost - Operational_Costs\n      Baseline_Profit = (nrow(test_data) * input$premium) - (actual_claims_val() * input$claim)\n      \n      Profit - Baseline_Profit  # Net savings\n    }, grid$cost_fn, grid$cost_fp)\n    \n    plot_ly(\n      data = grid,\n      x = ~cost_fn,\n      y = ~cost_fp,\n      z = ~net_savings,\n      type = \"contour\",\n      colorscale = \"Viridis\",\n      contours = list(showlabels = TRUE)\n    ) %&gt;%\n      layout(\n        title = \"2D Net Savings Sensitivity\",\n        xaxis = list(title = \"Cost per False Negative (SEK)\"),\n        yaxis = list(title = \"Cost per False Positive (SEK)\"),\n        annotations = list(\n          x = input$cost_fn,\n          y = input$cost_fp,\n          text = \"Current Values\",\n          showarrow = TRUE\n        )\n      )\n  })\n  \n  # ROC Curve\n  output$roc_curve = renderPlotly({\n    gg = ggplot() +\n      geom_path(aes(x = 1 - roc_curve$specificities, \n                   y = roc_curve$sensitivities),\n               color = \"#3498db\") +\n      geom_abline(slope = 1, intercept = 0, linetype = \"dashed\") +\n      geom_vline(xintercept = 1 - input$threshold, color = \"#e74c3c\") +\n      labs(title = \"ROC Curve\", \n          x = \"False Positive Rate (1 - Specificity)\", \n          y = \"True Positive Rate (Sensitivity)\") +\n      theme_minimal()\n    \n    ggplotly(gg) %&gt;% \n      layout(annotations = list(\n        x = 1 - input$threshold,\n        y = roc_curve$sensitivities[which.min(abs(roc_curve$thresholds - input$threshold))],\n        text = paste(\"Threshold =\", input$threshold),\n        showarrow = TRUE\n      ))\n  })\n  \n  # Confusion Matrix\n  output$conf_matrix = renderPlot({\n    cm_df = as.data.frame(conf_matrix()$table)\n    levels(cm_df$Reference) = c(\"No Claim\", \"Claim\")\n    levels(cm_df$Prediction) = c(\"No Claim\", \"Claim\")\n    \n    ggplot(cm_df, aes(Reference, Prediction, fill = Freq)) +\n      geom_tile(alpha = 0.8) +\n      geom_text(aes(label = paste0(c(\"TN\", \"FP\", \"FN\", \"TP\"), \"\\n\", Freq)), \n               color = \"white\", size = 6) +\n      scale_fill_gradient(low = \"#34495e\", high = \"#e74c3c\") +\n      labs(title = \"Confusion Matrix\", x = \"Actual\", y = \"Predicted\") +\n      theme_minimal()\n  })\n  \n  # Financial Breakdown\n  output$savings_breakdown = renderPlotly({\n    fin = financials()\n    plot_ly(\n      x = c(\"Revenue\", \"Claim Payouts\", \"FP Opportunity Cost\", \"Operational Costs\", \"Profit\"),\n      y = c(fin$Revenue, -fin$Claim_Payouts, -fin$FP_Cost, -fin$Operational_Costs, fin$Profit),\n      type = \"bar\",\n      marker = list(color = c('#3498db', '#e74c3c', '#e67e22', '#f39c12', '#2ecc71')),\n      text = c(\n        paste0(format(fin$Revenue, big.mark = \" \"), \" SEK\"),\n        paste0(format(-fin$Claim_Payouts, big.mark = \" \"), \" SEK\"),\n        paste0(format(-fin$FP_Cost, big.mark = \" \"), \" SEK\"),\n        paste0(format(-fin$Operational_Costs, big.mark = \" \"), \" SEK\"),\n        paste0(format(fin$Profit, big.mark = \" \"), \" SEK\")\n      ),\n      textposition = 'outside'\n    ) %&gt;%\n      layout(\n        title = \"Financial Breakdown (With Model)\",\n        yaxis = list(title = \"Amount (SEK)\"),\n        xaxis = list(title = \"\")\n      )\n  })\n  \n  # Metrics Table\n  output$metrics_table = renderDT({\n    fin = financials()\n    cm = conf_matrix()\n    \n    # Calculate percentages\n    accepted_pct = round(100 * (fin$TN + fin$FN) / nrow(test_data), 1)\n    rejected_pct = round(100 * (fin$TP + fin$FP) / nrow(test_data), 1)\n    \n    metrics = data.frame(\n      Metric = c(\"Threshold\", \n                \"Accuracy\", \n                \"Precision\", \n                \"Recall (Sensitivity)\",\n                \"AUC\",\n                \"Accepted Policies\",\n                \"Rejected Policies\",\n                \"Net Savings (vs Baseline)\",\n                \"Baseline Profit\",\n                \"Model Profit\"),\n      Value = c(\n        input$threshold,\n        round(cm$overall[\"Accuracy\"], 3),\n        round(cm$byClass[\"Precision\"], 3),\n        round(cm$byClass[\"Recall\"], 3),\n        round(auc(roc_curve), 3),\n        paste0(fin$TN + fin$FN, \" (\", accepted_pct, \"%)\"),\n        paste0(fin$TP + fin$FP, \" (\", rejected_pct, \"%)\"),\n        format(round(fin$Net_Savings), big.mark = \" \"),\n        format(round(fin$Baseline_Profit), big.mark = \" \"),\n        format(round(fin$Profit), big.mark = \" \")\n      )\n    )\n    \n    datatable(metrics, rownames = FALSE, \n              options = list(dom = 't', pageLength = 12)) %&gt;%\n      formatStyle('Value', fontWeight = 'bold')\n  })\n}\n\nshinyApp(ui, server)\n\n\nLet’s set input parameters to reflect a practical yearly scenario: risk threshold at 0.75, false negative (FN) cost = 6,300 SEK, false positive (FP) cost = 4,000 SEK, average premium = 3,000 SEK, and average claim payout = 12,000 SEK.\nIn the no-model baseline at a 0.75 risk threshold, all 961 actual claims are approved without any scrutiny, resulting in a total payout of 961 × 12,000 = 11,532,000 SEK. Premium revenue from all policies (3,000 × 3,000 = 9,000,000 SEK) falls short of covering these losses, yielding a baseline loss of 2,532,000 SEK. This represents the insurer’s maximum financial exposure without any risk modeling.\nAt the same 0.75 threshold, using our model, claim payouts significantly decrease as the insurer accepts only 2,402 policies (an 80.1% acceptance rate), generating premium revenue of 7,206,000 SEK. Claim payouts for misclassified high-risk clients total 495 × 12,000 = 5,940,000 SEK. Opportunity costs from 101 wrongly rejected safe clients amount to 404,000 SEK. Operational expenses add 951,600 SEK (15% of claim and opportunity costs). Total costs reach 7,295,600 SEK, resulting in a model profit of 316,400 SEK. Compared to the baseline loss, this generates 2,848,400 SEK in net savings, clearly demonstrating financial improvement with our model.\nBy rejecting 19.9% of applicants (596 drivers), the insurer avoids 466 high-risk policies that would have cost 5,592,000 SEK in claims. This showcases the model’s ability to strategically reallocate resources, converting potential 12,000 SEK liabilities into 4,000 SEK opportunity costs per avoided client. The trade-off here balances risk reduction with market retention, accepting 101 false positives to prevent 466 high-risk approvals. Performance-wise, the metrics validate this approach: an AUC of 0.901 demonstrates excellent risk discrimination, 83.1% precision indicates reliable risk identification, and a 0.636 F1-score reflects a balanced precision-recall ratio.\nWhile effective, the model isn’t yet profit-optimized for these parameters. Clicking “Find Optimal Threshold” shifts the threshold to 0.49, significantly improving financial outcomes. This adjustment increases risk detection recall from 51.5% to 80.5%, reducing false negatives from 495 to 189. Although false positives rise to 359, the net financial impact is transformative. Premium revenue is now 5,802,000 SEK, operational costs drop to 555,600 SEK, and model profit jumps to 1,878,200 SEK. Net savings increase to 4,410,200 SEK—a 55% improvement over the 0.75 threshold results. This improvement is a direct result of threshold optimization.\nThe 0.49 threshold prioritizes catching high-risk drivers (189 FNs versus 495 previously) while strategically accepting higher investigation costs (359 FPs versus 101). The change is also reflected in performance metrics: F1-score rises to 0.763 as recall improves significantly, while precision moderates to 72.6% as the model casts a wider net for high-risk cases. The incentive here is to solve the cost-minimizing, profit-maximizing problem. The dashboard enables this optimization through its profit-maximization algorithm, which tests thresholds to find the ideal balance where marginal risk-detection gains equal marginal opportunity costs.\nIt is safe to state that with the help of the dashboard, anyone can leverage predictive modeling to transform risk management. At a 0.75 threshold, the model turns a 2.5M SEK loss into modest profitability. If optimized, as in this case it generates substantial profits while controlling risk exposure. Visually, the 3D plot reveals profit-maximizing cost ratios, while the 2D plot identifies operational safe zones where profit varies less than 5% despite cost fluctuations. Which basically allow the users to make informed decisions based to their risk appetite and business goals."
  },
  {
    "objectID": "projects/predictive_risk_modeling.html#conclusion-and-next-steps",
    "href": "projects/predictive_risk_modeling.html#conclusion-and-next-steps",
    "title": "Predictive Risk Modeling for Auto Insurance: Claim Likelihood Estimation",
    "section": "",
    "text": "In this project, we developed a predictive risk modeling framework that accurately quantifies insurance claim probabilities in the auto insurance sector. Our framework retains all relevant predictors to ensure adherence with regulatory standards such as Solvency II, while simultaneously capturing compound risk patterns through interaction effects. We deployed the model through an interactive dashboard to optimize decision-making via cost-benefit analysis, while enabling stakeholders to test and simulate different scenarios by adjusting risk thresholds and parameters.\nTechnically, our model reveals strong performance on test data, with an AUC of 0.903 indicating excellent discriminatory ability to differentiate between risky and safe clients, complemented by a solid F-score score of 0.76 that reflects a balanced trade-off between precision and recall. With a simple modification, we were able to simplify the interpretation of the model by using odds ratios to identify key predictors that influence risk direction either positively or negatively (risk drivers or reducers) in an intuitive manner.\nThe most significant risk driver emerged as sports car drivers aged 65+, with 4.48 times higher claim likelihood, closely followed by male drivers at 2.65 times higher risk. In contrast, drivers with 20+ years of experience showed the strongest risk reduction effect.\nFinancially, this framework serves as the backbone of our interactive dashboard, allowing users to simulate strategies based on custom profiles and risk scenarios. This functionality empowers decision-makers through improved transparency and individualized risk assessment.\nIn our opinion, the next steps should be focused on strengthening the predictive power and flexibility of the model using more advanced machine learning methods. One example would be to integrate gradient boosting algorithms, such as XGBoost or LightGBM. This would allow us to capture non-linear relationships and high-order interactions that logistic regression may overlook, and would also remove the need for manual specification of interaction terms, offering a more adaptive approach to risk modeling."
  },
  {
    "objectID": "projects/predictive_risk_modeling.html#references",
    "href": "projects/predictive_risk_modeling.html#references",
    "title": "Predictive Risk Modeling for Auto Insurance: Claim Likelihood Estimation",
    "section": "",
    "text": "Hsu, Y., Chou, P., & Shiu, Y. (2016). An examination of the relationship between vehicle insurance purchase and the frequency of accidents. Asia Pacific Management Review, 21(4), 231–238. https://doi.org/10.1016/j.apmrv.2016.08.001\nMark Goldburd, Khare, A., Tevet, D., Guller, D., & Casualty Actuarial Society. (2020). GENERALIZED LINEAR MODELS FOR INSURANCE RATING (Second Edition). Casualty Actuarial Society."
  },
  {
    "objectID": "projects/predictive_risk_modeling.html",
    "href": "projects/predictive_risk_modeling.html",
    "title": "Predictive Risk Modeling for Auto Insurance: Claim Likelihood Estimation",
    "section": "",
    "text": "Insurance companies operate in a world driven by probabilities, where every decision is based on a pile of aggregate information. This project aims to develop a logistic regression model capable of accurately classifying clients based on their likelihood of filing a claim and their level of risk. Beyond its technical purpose, this project also serves as a tool to bring some clarity to non-technical audiences on the often unclear process of avoiding costly financial losses, such as mistakenly classifying a high-risk client as low-risk with the help of statistical tools.\nThe data used for this project comes from the publicly available Car Insurance Data on Kaggle (kaggle.com/dataset/car-insurance-data), which contains information on car insurance claims. This model is not only beneficial for analysts but also serves as a helpful resource for underwriters, helping to reduce or avoid potential errors. The model doesn’t just predict risk; it also aligns with Solvency II standards, ensuring transparency and trust in every result."
  },
  {
    "objectID": "projects/predictive_risk_modeling.html#dataset-overview",
    "href": "projects/predictive_risk_modeling.html#dataset-overview",
    "title": "Predictive Risk Modeling for Auto Insurance: Claim Likelihood Estimation",
    "section": "2.1 Dataset Overview",
    "text": "2.1 Dataset Overview\n\n\nCode\ndata_car = import(\"/Users/Brook/Downloads/Car_Insurance_Claim.csv\")\nhead(data_car, 5)\n\n\n      ID   AGE GENDER     RACE DRIVING_EXPERIENCE   EDUCATION        INCOME\n1 569520   65+ female majority               0-9y high school   upper class\n2 750365 16-25   male majority               0-9y        none       poverty\n3 199901 16-25 female majority               0-9y high school working class\n4 478866 16-25   male majority               0-9y  university working class\n5 731664 26-39   male majority             10-19y        none working class\n  CREDIT_SCORE VEHICLE_OWNERSHIP VEHICLE_YEAR MARRIED CHILDREN POSTAL_CODE\n1    0.6290273                 1   after 2015       0        1       10238\n2    0.3577571                 0  before 2015       0        0       10238\n3    0.4931458                 1  before 2015       0        0       10238\n4    0.2060129                 1  before 2015       0        1       32765\n5    0.3883659                 1  before 2015       0        0       32765\n  ANNUAL_MILEAGE VEHICLE_TYPE SPEEDING_VIOLATIONS DUIS PAST_ACCIDENTS OUTCOME\n1          12000        sedan                   0    0              0       0\n2          16000        sedan                   0    0              0       1\n3          11000        sedan                   0    0              0       0\n4          11000        sedan                   0    0              0       0\n5          12000        sedan                   2    0              1       1\n\n\n\n\nCode\n# Detailed summary of the missing values, counts and percentage, data type of the predictors\n\ndfSummary(data_car) \n\n\nData Frame Summary  \ndata_car  \nDimensions: 10000 x 19  \nDuplicates: 0  \n\n--------------------------------------------------------------------------------------------------------------------------------\nNo   Variable              Stats / Values                    Freqs (% of Valid)      Graph                  Valid      Missing  \n---- --------------------- --------------------------------- ----------------------- ---------------------- ---------- ---------\n1    ID                    Mean (sd) : 500521.9 (290030.8)   10000 distinct values   : . : . : : : . : :    10000      0        \n     [integer]             min &lt; med &lt; max:                                          : : : : : : : : : :    (100.0%)   (0.0%)   \n                           101 &lt; 501777 &lt; 999976                                     : : : : : : : : : :                        \n                           IQR (CV) : 504336 (0.6)                                   : : : : : : : : : :                        \n                                                                                     : : : : : : : : : :                        \n\n2    AGE                   1. 16-25                          2016 (20.2%)            IIII                   10000      0        \n     [character]           2. 26-39                          3063 (30.6%)            IIIIII                 (100.0%)   (0.0%)   \n                           3. 40-64                          2931 (29.3%)            IIIII                                      \n                           4. 65+                            1990 (19.9%)            III                                        \n\n3    GENDER                1. female                         5010 (50.1%)            IIIIIIIIII             10000      0        \n     [character]           2. male                           4990 (49.9%)            IIIIIIIII              (100.0%)   (0.0%)   \n\n4    RACE                  1. majority                       9012 (90.1%)            IIIIIIIIIIIIIIIIII     10000      0        \n     [character]           2. minority                        988 ( 9.9%)            I                      (100.0%)   (0.0%)   \n\n5    DRIVING_EXPERIENCE    1. 0-9y                           3530 (35.3%)            IIIIIII                10000      0        \n     [character]           2. 10-19y                         3299 (33.0%)            IIIIII                 (100.0%)   (0.0%)   \n                           3. 20-29y                         2119 (21.2%)            IIII                                       \n                           4. 30y+                           1052 (10.5%)            II                                         \n\n6    EDUCATION             1. high school                    4157 (41.6%)            IIIIIIII               10000      0        \n     [character]           2. none                           1915 (19.1%)            III                    (100.0%)   (0.0%)   \n                           3. university                     3928 (39.3%)            IIIIIII                                    \n\n7    INCOME                1. middle class                   2138 (21.4%)            IIII                   10000      0        \n     [character]           2. poverty                        1814 (18.1%)            III                    (100.0%)   (0.0%)   \n                           3. upper class                    4336 (43.4%)            IIIIIIII                                   \n                           4. working class                  1712 (17.1%)            III                                        \n\n8    CREDIT_SCORE          Mean (sd) : 0.5 (0.1)             9018 distinct values              : .          9018       982      \n     [numeric]             min &lt; med &lt; max:                                                  : : :          (90.2%)    (9.8%)   \n                           0.1 &lt; 0.5 &lt; 1                                                   : : : :                              \n                           IQR (CV) : 0.2 (0.3)                                          . : : : : .                            \n                                                                                       . : : : : : : .                          \n\n9    VEHICLE_OWNERSHIP     Min  : 0                          0 : 3030 (30.3%)        IIIIII                 10000      0        \n     [numeric]             Mean : 0.7                        1 : 6970 (69.7%)        IIIIIIIIIIIII          (100.0%)   (0.0%)   \n                           Max  : 1                                                                                             \n\n10   VEHICLE_YEAR          1. after 2015                     3033 (30.3%)            IIIIII                 10000      0        \n     [character]           2. before 2015                    6967 (69.7%)            IIIIIIIIIIIII          (100.0%)   (0.0%)   \n\n11   MARRIED               Min  : 0                          0 : 5018 (50.2%)        IIIIIIIIII             10000      0        \n     [numeric]             Mean : 0.5                        1 : 4982 (49.8%)        IIIIIIIII              (100.0%)   (0.0%)   \n                           Max  : 1                                                                                             \n\n12   CHILDREN              Min  : 0                          0 : 3112 (31.1%)        IIIIII                 10000      0        \n     [numeric]             Mean : 0.7                        1 : 6888 (68.9%)        IIIIIIIIIIIII          (100.0%)   (0.0%)   \n                           Max  : 1                                                                                             \n\n13   POSTAL_CODE           Mean (sd) : 19864.5 (18915.6)     10238 : 6940 (69.4%)    IIIIIIIIIIIII          10000      0        \n     [integer]             min &lt; med &lt; max:                  21217 :  120 ( 1.2%)                           (100.0%)   (0.0%)   \n                           10238 &lt; 10238 &lt; 92101             32765 : 2456 (24.6%)    IIII                                       \n                           IQR (CV) : 22527 (1)              92101 :  484 ( 4.8%)                                               \n\n14   ANNUAL_MILEAGE        Mean (sd) : 11697 (2818.4)        21 distinct values              :              9043       957      \n     [numeric]             min &lt; med &lt; max:                                                : : :            (90.4%)    (9.6%)   \n                           2000 &lt; 12000 &lt; 22000                                            : : :                                \n                           IQR (CV) : 4000 (0.2)                                         : : : : :                              \n                                                                                       . : : : : : .                            \n\n15   VEHICLE_TYPE          1. sedan                          9523 (95.2%)            IIIIIIIIIIIIIIIIIII    10000      0        \n     [character]           2. sports car                      477 ( 4.8%)                                   (100.0%)   (0.0%)   \n\n16   SPEEDING_VIOLATIONS   Mean (sd) : 1.5 (2.2)             21 distinct values      :                      10000      0        \n     [integer]             min &lt; med &lt; max:                                          :                      (100.0%)   (0.0%)   \n                           0 &lt; 0 &lt; 22                                                :                                          \n                           IQR (CV) : 2 (1.5)                                        :                                          \n                                                                                     : : .                                      \n\n17   DUIS                  Mean (sd) : 0.2 (0.6)             0 : 8118 (81.2%)        IIIIIIIIIIIIIIII       10000      0        \n     [integer]             min &lt; med &lt; max:                  1 : 1470 (14.7%)        II                     (100.0%)   (0.0%)   \n                           0 &lt; 0 &lt; 6                         2 :  331 ( 3.3%)                                                   \n                           IQR (CV) : 0 (2.3)                3 :   68 ( 0.7%)                                                   \n                                                             4 :   10 ( 0.1%)                                                   \n                                                             5 :    2 ( 0.0%)                                                   \n                                                             6 :    1 ( 0.0%)                                                   \n\n18   PAST_ACCIDENTS        Mean (sd) : 1.1 (1.7)             15 distinct values      :                      10000      0        \n     [integer]             min &lt; med &lt; max:                                          :                      (100.0%)   (0.0%)   \n                           0 &lt; 0 &lt; 15                                                :                                          \n                           IQR (CV) : 2 (1.6)                                        :                                          \n                                                                                     : :   .                                    \n\n19   OUTCOME               Min  : 0                          0 : 6867 (68.7%)        IIIIIIIIIIIII          10000      0        \n     [numeric]             Mean : 0.3                        1 : 3133 (31.3%)        IIIIII                 (100.0%)   (0.0%)   \n                           Max  : 1                                                                                             \n--------------------------------------------------------------------------------------------------------------------------------\n\n\nWe can see that data has been imported correctly, and the quality of the data has been thoroughly examined. The dataset consists of 10,000 drivers, where most of the variables appear to be clean, except for CREDIT_SCORE and ANNUAL_MILEAGE, each with around 10% missing values. This is not necessarily a bad thing, as the issue can be addressed using imputation. A common technique to replace missing values with estimated or substituted values. Imputation not only completes the dataset but also helps mitigate potential biases and gaps in information before conducting our analysis. Since the proportion of missing values is relatively small, we will use median imputation, which is a widely accepted practice in the field.\nOur target variable (OUTCOME) shows a moderate class imbalance (68.7% no claims vs. 31.3% claims), which we will address later using weighted GLM to mitigate the issue associated with such imbalance. Looking at key predictors such as VEHICLE_TYPE (4.8% high-risk sports cars), AGE (20.2% young drivers), and PAST_ACCIDENTS (maximum of 15 accidents), we can observe some risk patterns. Furthermore, we are dropping ID variable as it has no predictive value, as for the POSTAL_CODE variable, we are dropping it as well because it shows low variability (69.4%) and would likely add noise without contributing to a meaningful predictive capability. Especially considering that real-world postal codes often correlate with risk factors, such as accident-prone areas\nBased on the data review, this are the steps that are needed before producing our model, we need to handle the missing values, do something about outlier and lastly managing class imbalance for the target variable."
  },
  {
    "objectID": "projects/predictive_risk_modeling.html#missing-value-treatment",
    "href": "projects/predictive_risk_modeling.html#missing-value-treatment",
    "title": "Predictive Risk Modeling for Auto Insurance: Claim Likelihood Estimation",
    "section": "2.2 Missing Value Treatment",
    "text": "2.2 Missing Value Treatment\n\n\nCode\n# Dropping POSTAL_CODE and ID Variable \ndata_car = data_car[, c(-1, -13)]\n\n# Visuals over missing values in bar plot  \ngg_miss_var(data_car) + geom_col(aes(y = n_miss, fill = n_miss &gt; 0), color = \"black\", linewidth = 0.3) +  labs(subtitle = \"CREDIT_SCORE and ANNUAL_MILEAGE have missing values (~10%)\"\n  ) +\n  theme_classic(base_size = 12)\n\n\n\n\n\n\n\nCode\n# Imputing, using median \ndata_car  =  data_car %&gt;%  \n  mutate(  \n    CREDIT_SCORE = ifelse(is.na(CREDIT_SCORE), median(CREDIT_SCORE, na.rm = TRUE), CREDIT_SCORE),  \n    ANNUAL_MILEAGE = ifelse(is.na(ANNUAL_MILEAGE), median(ANNUAL_MILEAGE, na.rm = TRUE), ANNUAL_MILEAGE) )  \n\n# Checking if there's missing values in each variable\ncolSums(is.na(data_car))\n\n\n                AGE              GENDER                RACE  DRIVING_EXPERIENCE \n                  0                   0                   0                   0 \n          EDUCATION              INCOME        CREDIT_SCORE   VEHICLE_OWNERSHIP \n                  0                   0                   0                   0 \n       VEHICLE_YEAR             MARRIED            CHILDREN      ANNUAL_MILEAGE \n                  0                   0                   0                   0 \n       VEHICLE_TYPE SPEEDING_VIOLATIONS                DUIS      PAST_ACCIDENTS \n                  0                   0                   0                   0 \n            OUTCOME \n                  0"
  },
  {
    "objectID": "projects/predictive_risk_modeling.html#outlier-detection-and-handling",
    "href": "projects/predictive_risk_modeling.html#outlier-detection-and-handling",
    "title": "Predictive Risk Modeling for Auto Insurance: Claim Likelihood Estimation",
    "section": "2.3 Outlier Detection and Handling",
    "text": "2.3 Outlier Detection and Handling\nAnother necessary step to consider, especially when modeling probabilities is Outliers. Which has the ability to misrepresent the relationship between predictors and the target variable. If extreme values in predictors like ANNUAL_MILEAGE or PAST_ACCIDENTS exist, they might disproportionately influence the model’s weights, leading to biased odds ratios by inflating coefficients and producing misleading conclusions. From a business perspective, consider an extreme case like 22 speeding violations in the SPEEDING_VIOLATIONS variable. Cases like 22 speeding violations may indicate data errors or fraud, which would skew risk assessments.\n\n\nCode\nnumeric_var = c(\"PAST_ACCIDENTS\", \"SPEEDING_VIOLATIONS\", \"DUIS\", \"ANNUAL_MILEAGE\")  \n\n# Density distributions for all relevant variables\ndistribution_fig = ggplot(data_car %&gt;% pivot_longer(all_of(numeric_var)), aes(value, fill = name)) +  \n  geom_density(alpha = 0.6) +  \n  facet_wrap(~name, scales = \"free\", ncol = 2) +  \n  labs(  \n    title = \"Distribution of Key Variables (Before Capping)\",  \n    subtitle = \"Right-skewed distributions with extreme values (e.g., 22 speeding violations)\"  \n  ) +  \n  theme_classic() +  \n  theme(legend.position = \"none\") \n\ndistribution_fig \n\n\n\n\n\nBased on the distributions shown in the plot, capping DUIS, PAST_ACCIDENTS, and SPEEDING_VIOLATIONS appears reasonable, as these variables showcases a strong right skew with extreme values. The ANNUAL_MILEAGE variable, on the other hand, appears approximately normally distributed, making capping unnecessary.\nTo alleviate the impact of Outliers on the model’s coefficient estimates, we use a capping method. Capping helps to avoid biases while preserving data size and statistical power, which is important in a time-constrained project as ours. The variables targeted for capping are of a numeric nature. Mainly DUIS, PAST_ACCIDENTS, and SPEEDING_VIOLATIONS distribution, which follows a exponential distribution (see distribution figure), we would use a capping to reduce the exponential effect on our model. Variables in questions are PAST_ACCIDENTS,SPEEDING_VIOLATIONS, DUIS.\n\n\nCode\n# Capping function with thresholds \ncap_custom = function(x, variable_n) { \n  \n  threshold = case_when(  \n    variable_n == \"DUIS\" ~ 3,  \n    variable_n == \"PAST_ACCIDENTS\" ~ 4,  \n    variable_n == \"SPEEDING_VIOLATIONS\" ~ 6,  \n    TRUE ~ max(x, na.rm = TRUE)  # while no cap for the other variables  \n  )  \n  pmin(x, threshold)  \n}  \n\n# Then applying it on the dataset\ndata_car = data_car %&gt;%  \n  mutate(  \n    DUIS = cap_custom(DUIS, \"DUIS\"),  \n    PAST_ACCIDENTS = cap_custom(PAST_ACCIDENTS, \"PAST_ACCIDENTS\"),  \n    SPEEDING_VIOLATIONS = cap_custom(SPEEDING_VIOLATIONS, \"SPEEDING_VIOLATIONS\")  \n  )  \n\n# Distribution figure after capping\nggplot(data_car %&gt;%  \n         pivot_longer(c(DUIS, PAST_ACCIDENTS, SPEEDING_VIOLATIONS)),  \n       aes(value, fill = name)) +  \n  geom_density(alpha = 0.6) +  \n  facet_wrap(~name, scales = \"free\", ncol = 3) +  \n  labs(  \n    title = \"Distributions After Threshold Capping\",  \n    subtitle = \"DUIS ≤2 | Accidents ≤4 | Speeding Violations ≤6\"  \n  ) +  \n  theme_classic() +  \n  theme(legend.position = \"none\")  \n\n\n\n\n\nBased on the figure above, the thresholds were strategically determined based on distribution analysis (see figure 1). Specifically, a threshold of 2 was implemented for DUIS. This balances the need to capture drivers with multiple offenses while limiting the impact of outlier values on the model. Given the observed distribution of PAST_ACCIDENTS, a threshold of 4 was chosen, acknowledging that most drivers have relatively few prior accidents. Lastly, for SPEEDING_VIOLATIONS, a threshold of 6 was selected, with intention to reflect a balance between capturing risky driving behavior and reducing the effects of extreme, potentially extreme values."
  },
  {
    "objectID": "projects/predictive_risk_modeling.html#feature-engineering",
    "href": "projects/predictive_risk_modeling.html#feature-engineering",
    "title": "Predictive Risk Modeling for Auto Insurance: Claim Likelihood Estimation",
    "section": "2.4 Feature Engineering",
    "text": "2.4 Feature Engineering\nBefore building the model, we need to refine the dataset further, starting with addressing the class imbalance in the target variable (OUTCOME). A common issue with class imbalances, particularly when it involves the target variable, is that most machine learning models tend to prioritize predicting the majority class. In this case, the majority class is non-claims as the image below shows, and models often focus on it to maximize overall accuracy. Meaning that with uneven class distribution, the model will “cheat” by showing biasness toward majority class (no claims) to maximize its accuracy while ignoring the minority, from a business perspective, it would mean ignoring high risk drivers.\nStatistically, this can lead to misclassifying high-risk drivers as low-risk (false negatives, FN), which could result in financial losses for the insurer. We will address this using techniques like class weighting (Weighted GLM) to ensure the model does not overlook critical risk patterns.\nFor example, a driver who is 34 years old, drives a sports car, and has had four past accidents would be considered high-risk. If such a driver is classified as low-risk by the model, it could lead to significant financial losses for the insurer due to missed risk assessments. Without corrections to address the imbalance, such cases could be missed frequently and insurer will be approving risky policies due to biased model.\n\n2.4.1 Class Imbalance Handling\n\n\nCode\n# Plot class distribution\nclass_dist = ggplot(data_car, aes(x = factor(OUTCOME), fill = factor(OUTCOME))) +\n  geom_bar() +\n  scale_fill_manual(values = c(\"blue\", \"red\")) +\n  labs(\n    title = \"Class Distribution of Insurance Claims\",\n    x = \"Claim Status (0 = No Claim, 1 = Claim)\",\n    y = \"Count\",\n    fill = \"Outcome\"\n  ) +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\nclass_dist\n\n\n\n\n\nThe plot above addresses the class imbalance. It appears that the majority class has over twice as much observations as the minority class, to be precis a class ratio: 68.7% (no claim) vs. 31.3% (claim), a 2.2:1 imbalance. By adding weights to the minority class the model will reduce incidents of false negative (risky drivers being approved) but mostly likely increase false positives (safe drivers being rejected, FP) with the main priority here from insurer point of view is to minimize financial losses over lost revenue due to rejecting safe drivers. the selection of the optimal weights will be based on the F-score, a metric often used to balance between precision and recall, and in our case maximizing the F-score helps ensure we accurately identify high-risk drivers without overly flagging low-risk ones.\n\n\nCode\n# we define and test diff weights,  1.5 - 2.5\nweights_to_test = seq(1.5, 2.5, by = 0.1)\n\n# Store\nresults = data.frame(weight = weights_to_test, f_score = numeric(length(weights_to_test)))\n\n# Loop and then evaluate F-scores\nfor (i in seq_along(weights_to_test)) {\n  model = glm(\n    OUTCOME ~ .,\n    data = data_car,\n    family = \"binomial\",\n    weights = ifelse(OUTCOME == 1, weights_to_test[i], 1)\n  )\n  predictions = ifelse(predict(model, type = \"response\") &gt; 0.5, 1, 0)\n  cm = confusionMatrix(factor(predictions, levels = c(0, 1)), factor(data_car$OUTCOME, levels = c(0, 1)))\n  results$f_score[i] = cm$byClass[\"F1\"]  \n}\n\n# Selecting the optimal weight with maximum F-score\noptimal_weight = results$weight[which.max(results$f_score)]\noptimal_f1 = max(results$f_score)\n\n\n# Plot results with highlighted optimal weight\nggplot(results, aes(weight, f_score)) +\n  geom_line(color = \"black\") +\n  geom_point(color = \"orange\") +\n  geom_point(data = results[which.max(results$f_score), ], aes(weight, f_score),\n             color = \"red\", size = 4) + \n  labs(\n    title = \"Optimal Weight for Class Balancing\",\n    x = \"Weight for Claims (OUTCOME=1)\",\n    y = \"F-score\"\n  ) +\n  theme_classic()\n\n\n\n\n\nNow, we have obtained the optimal weight based on the F-score, which will be used when we train the model.\n\n\n2.4.2 Variable selection\nnow, to ensure transparency, we will be using all the predictors in the logistic model. A step to align with best practices for interpretable insurance risk modeling, where domain knowledge and regulatory compliance prioritize comprehensive risk assessment over algorithmic simplicity.\nOur rationale includes:\n\nRegulatory Requirements: Solvency II mandates insurers to justify all risk drivers, even marginally significant ones.\nField knowledge: Different variables like EDUCATION and RACE may directly or indirectly influence risk, but at the same time provides socioeconomic context.\nResearch alignment: Hsu et al. (2016b) emphasize interaction terms to capture compounding risk effects.\n\nIn this step, we are creating a couple of new variables with a clear intention to capture the synergistic effects between variables, meaning the combined impact of two predictors. Which could be considered greater than their individual effects.This addresses the multiplicative nature of insurance risk (Goldburd et al., 2020), where risks is rarely additive rather they tend to be additive."
  },
  {
    "objectID": "projects/predictive_risk_modeling.html#logistic-regression-coefficients",
    "href": "projects/predictive_risk_modeling.html#logistic-regression-coefficients",
    "title": "Predictive Risk Modeling for Auto Insurance: Claim Likelihood Estimation",
    "section": "3.1 Logistic Regression coefficients",
    "text": "3.1 Logistic Regression coefficients\nThis section presents our model applied to the training dataset, together with key interaction terms. The model quantifies how demographic, behavioral, and vehicle-related factors influence claim likelihood, with a special focus on the interactions “AGE:VEHICLE_TYPE” and “PAST_ACCIDENTS:SPEEDING_VIOLATIONS.\nFor easier interpretation, we transform the estimated coefficients into odds ratios and visualize these ratios. The plot below categorizes predictors as risk drivers (red) or risk reducers (green), using asterisks (*) to denote statistical significance. This approach could be useful for insurers to identify opportunities to adjust premiums for high-risk groups or to reward safer behaviors.\n\n\nCode\n# Final Model (Training Data)\nfinal_model = glm(\n  OUTCOME ~ AGE + GENDER + RACE + DRIVING_EXPERIENCE + EDUCATION + INCOME +\n    CREDIT_SCORE + VEHICLE_OWNERSHIP + VEHICLE_YEAR + MARRIED + CHILDREN +\n    ANNUAL_MILEAGE + VEHICLE_TYPE + SPEEDING_VIOLATIONS + DUIS + PAST_ACCIDENTS +\n    AGE:VEHICLE_TYPE + PAST_ACCIDENTS:SPEEDING_VIOLATIONS,\n  data = train_data,\n  family = \"binomial\",\n  weights = ifelse(OUTCOME == 1, optimal_weight, 1)\n)\n\n# summary\nmodelsummary(final_model, output = \"markdown\", stars = T)\n\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                (1)\n              \n        \n        + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n        \n                \n                  (Intercept)                         \n                  1.612*** \n                \n                \n                                                      \n                  (0.256)  \n                \n                \n                  AGE26-39                            \n                  -0.223*  \n                \n                \n                                                      \n                  (0.109)  \n                \n                \n                  AGE40-64                            \n                  -0.107   \n                \n                \n                                                      \n                  (0.127)  \n                \n                \n                  AGE65+                              \n                  -0.164   \n                \n                \n                                                      \n                  (0.160)  \n                \n                \n                  GENDERmale                          \n                  0.974*** \n                \n                \n                                                      \n                  (0.068)  \n                \n                \n                  RACEminority                        \n                  -0.264*  \n                \n                \n                                                      \n                  (0.106)  \n                \n                \n                  DRIVING_EXPERIENCE10-19y            \n                  -1.578***\n                \n                \n                                                      \n                  (0.112)  \n                \n                \n                  DRIVING_EXPERIENCE20-29y            \n                  -3.208***\n                \n                \n                                                      \n                  (0.179)  \n                \n                \n                  DRIVING_EXPERIENCE30y+              \n                  -3.636***\n                \n                \n                                                      \n                  (0.287)  \n                \n                \n                  EDUCATIONhigh school                \n                  -0.029   \n                \n                \n                                                      \n                  (0.088)  \n                \n                \n                  EDUCATIONuniversity                 \n                  -0.179+  \n                \n                \n                                                      \n                  (0.105)  \n                \n                \n                  INCOMEworking class                 \n                  0.089    \n                \n                \n                                                      \n                  (0.103)  \n                \n                \n                  INCOMEmiddle class                  \n                  -0.015   \n                \n                \n                                                      \n                  (0.124)  \n                \n                \n                  INCOMEupper class                   \n                  0.008    \n                \n                \n                                                      \n                  (0.156)  \n                \n                \n                  CREDIT_SCORE                        \n                  -0.145   \n                \n                \n                                                      \n                  (0.343)  \n                \n                \n                  VEHICLE_OWNERSHIPYes                \n                  -1.599***\n                \n                \n                                                      \n                  (0.072)  \n                \n                \n                  VEHICLE_YEARafter 2015              \n                  -1.695***\n                \n                \n                                                      \n                  (0.085)  \n                \n                \n                  MARRIEDYes                          \n                  -0.312***\n                \n                \n                                                      \n                  (0.074)  \n                \n                \n                  CHILDRENYes                         \n                  -0.163*  \n                \n                \n                                                      \n                  (0.073)  \n                \n                \n                  ANNUAL_MILEAGE                      \n                  0.060*** \n                \n                \n                                                      \n                  (0.014)  \n                \n                \n                  VEHICLE_TYPEsports car              \n                  -0.323   \n                \n                \n                                                      \n                  (0.287)  \n                \n                \n                  SPEEDING_VIOLATIONS                 \n                  0.073+   \n                \n                \n                                                      \n                  (0.039)  \n                \n                \n                  DUIS                                \n                  0.035    \n                \n                \n                                                      \n                  (0.077)  \n                \n                \n                  PAST_ACCIDENTS                      \n                  -0.312***\n                \n                \n                                                      \n                  (0.057)  \n                \n                \n                  AGE26-39 × VEHICLE_TYPEsports car   \n                  0.559    \n                \n                \n                                                      \n                  (0.356)  \n                \n                \n                  AGE40-64 × VEHICLE_TYPEsports car   \n                  0.831*   \n                \n                \n                                                      \n                  (0.390)  \n                \n                \n                  AGE65+ × VEHICLE_TYPEsports car     \n                  1.500**  \n                \n                \n                                                      \n                  (0.522)  \n                \n                \n                  SPEEDING_VIOLATIONS × PAST_ACCIDENTS\n                  0.009    \n                \n                \n                                                      \n                  (0.020)  \n                \n                \n                  Num.Obs.                            \n                  7000     \n                \n                \n                  AIC                                 \n                  7558.4   \n                \n                \n                  BIC                                 \n                  7750.3   \n                \n                \n                  Log.Lik.                            \n                  -3751.203\n                \n                \n                  F                                   \n                  77.684   \n                \n                \n                  RMSE                                \n                  0.34     \n                \n        \n      \n    \n\n\n\n\n\nCode\n# plot Risk driver vs. risk reducer predictors. Odds}\n# Assume final_model is your fitted logistic regression model\nmodel_summary &lt;- tidy(final_model)\n\nodds_table &lt;- data.frame(\n  Predictor = names(exp(coef(final_model))),\n  Odds_Ratio = round(exp(coef(final_model)), 2),\n  p_value = model_summary$p.value\n) %&gt;%\n  filter(Predictor != \"(Intercept)\") %&gt;%\n  mutate(\n    Effect_Type = ifelse(Odds_Ratio &gt; 1, \"Risk Driver\", \"Risk Reducer\"),\n    Predictor_Label = ifelse(\n      p_value &lt; 0.1,\n      paste0(\"&lt;b&gt;\", Predictor, \"&lt;/b&gt;\"),  # Bold significant predictors\n      Predictor\n    ),\n    Odds_Label = case_when(\n      p_value &lt; 0.01 ~ paste0(Odds_Ratio, \"***\"),\n      p_value &lt; 0.05 ~ paste0(Odds_Ratio, \"**\"),\n      p_value &lt; 0.1 ~ paste0(Odds_Ratio, \"*\"),\n      TRUE ~ as.character(Odds_Ratio)\n    )\n  ) %&gt;%\n  arrange(desc(Odds_Ratio))\n\n# Calculate maximum label width for dynamic margin\nmax_label_width &lt;- max(nchar(odds_table$Predictor_Label)) * 0.1\n\nodds_plot &lt;- ggplot(odds_table, \n                    aes(x = reorder(Predictor_Label, Odds_Ratio), \n                        y = Odds_Ratio, \n                        fill = Effect_Type)) +\n  geom_col(alpha = 0.8, width = 0.7) +\n  geom_hline(yintercept = 1, linetype = \"dashed\", color = \"gray40\") +\n  geom_text(\n    aes(label = Odds_Label),\n    hjust = -0.2, \n    size = 3.5,\n    position = position_nudge(y = 0.05) # Small nudge to avoid bar collision\n  ) +\n  coord_flip(clip = \"off\") + # Allow labels to extend beyond panel\n  scale_fill_manual(values = c(\"Risk Driver\" = \"#E74C3C\", \"Risk Reducer\" = \"#2ECC71\")) +\n  labs(\n    title = \"All Predictors: Significant Risk Drivers/Reducers\",\n    subtitle = \"Odds Ratio &gt; 1 = Higher Risk | * = p &lt; 0.1, ** = p &lt; 0.05, *** = p &lt; 0.01\",\n    x = \"Predictor\",\n    y = \"Odds Ratio (Reference = 1)\",\n    fill = \"Effect Type\"\n  ) +\n  theme_classic() +\n  theme(\n    legend.position = \"bottom\",\n    axis.text.y = element_markdown(size = 10, color = \"black\"),\n    plot.margin = margin(20, max_label_width * 10 + 60, 20, 20), # Dynamic right margin\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 10)\n  ) +\n  scale_y_continuous(\n    expand = expansion(mult = c(0, 0.2)), # Extra space on top for labels\n    limits = c(0, max(odds_table$Odds_Ratio) * 1.3)\n  )\n\nodds_plot\n\n\n\n\n\nResults show significant variation in predictors’ statistical power. Odds ratios serve as claim likelihood multipliers, where a value of one indicates no effect relative to the reference group, values above one suggest increased odds, and values below one indicate reduced odds. For example, an odds ratio of two means double the claim odds compared to the reference group. All interpretations are relative to baseline categories (e.g., female for gender, youngest age group, least experienced drivers).\nObserving the plot, there’s three main risk drivers that emerge: Individuals aged 65+ driving sports cars exhibit 4.48 times higher claim odds than the reference group, representing the strongest risk driver. Male drivers show 2.65 times higher odds than females. The 40-64 age group driving sports cars demonstrates 2.3 times higher claim odds. These results are consistent with industry expectations, as both sports cars and certain demographic traits are well-known to increase risk, and a combination of these factors clearly amplifies the effect.\nOn the other hand, the main risk reducers include 30+ years of driving experience (97% lower claims), 20-29 years of experience (96% lower claims), and vehicles produced after 2015 (82% lower claims). It appears that possessing driving experience and newer vehicle has a protective effect on filing a claim. Remarkably, past accidents correlate with 27% lower claims, suggesting behavioral changes post-accident, while marriage similarly reduces odds by 27%.\nOne notable and somewhat surprising result is that past accidents is correlate with 27% lower claims, indicating a post-accident behavioral shift. The same is observable in regard to marriage, as it correlate with 27% lower claim. Overall, we can say that the model performs as expected, as these findings provide clear guidance for targeted premium adjustments, ensuring that pricing reflects the actual risk profile revealed by the data."
  },
  {
    "objectID": "projects/predictive_risk_modeling.html#shap-value",
    "href": "projects/predictive_risk_modeling.html#shap-value",
    "title": "Predictive Risk Modeling for Auto Insurance: Claim Likelihood Estimation",
    "section": "3.2 SHAP Value",
    "text": "3.2 SHAP Value\nPreviously, we explained how each predictor affects the odds of an insurance claim, showing whether a factor increases or decreases risk. Now, we focus on a different but equally important question: Which variables have the biggest impact on the model’s predictions, and by how much do they influence the outcome for each case? This is where SHAP values are valuable. SHAP values measure how much each predictor contributes, on average, to increasing or decreasing the model’s output for individual cases. A higher mean SHAP value means that feature has a greater overall impact on the model’s predictions, regardless of whether it increases or decreases the risk. Giving us a clearer and more practical understanding of how the model makes its prediction.\n\n\nCode\nexplainer = DALEX::explain(\n  model = final_model,\n  data  = as.data.frame(train_data)[ , setdiff(names(train_data), \"OUTCOME\") ],\n  # Convert factor to 0/1 on the fly:\n  y     = as.numeric(train_data$OUTCOME) - 1,\n  # Default predict_function will return log-odds; we want probabilities:\n  predict_function = function(m, newdata) {\n    predict(m, newdata = newdata, type = \"response\")\n  },\n  label = \"Logistic Model\"\n)\n\n\nPreparation of a new explainer is initiated\n  -&gt; model label       :  Logistic Model \n  -&gt; data              :  7000  rows  16  cols \n  -&gt; target variable   :  7000  values \n  -&gt; predict function  :  function(m, newdata) {     predict(m, newdata = newdata, type = \"response\") } \n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package stats , ver. 4.2.2 , task classification (  default  ) \n  -&gt; predicted values  :  numerical, min =  0.001020305 , mean =  0.358588 , max =  0.9759593  \n  -&gt; residual function :  difference between y and yhat (  default  )\n  -&gt; residuals         :  numerical, min =  -0.9713287 , mean =  -0.0483023 , max =  0.9944156  \n  A new explainer has been created!  \n\n\nCode\nshap_values = predict_parts(\n  explainer,\n  new_observation = train_data[1:100, ],\n  type            = \"shap\",\n  B               = 100\n)\n\nshap_aggregated = shap_values %&gt;%\n  group_by(variable) %&gt;%\n  summarize(mean_abs_shap = mean(abs(contribution)), .groups = \"drop\") %&gt;%\n  arrange(desc(mean_abs_shap))\n\n# \nggplot(shap_aggregated, aes(\n    x = reorder(variable, mean_abs_shap),\n    y = mean_abs_shap\n  )) +\n  geom_col(fill = \"#2F4F4F\", width = 0.7) +\n  coord_flip() +\n  labs(\n    title    = \"Global Feature Importance: SHAP Values\",\n    subtitle = \"Top Drivers of Insurance Claim Predictions\",\n    x        = \"Predictors\",\n    y        = \"Mean |SHAP Contribution|\"\n  ) +\n  theme_classic() +\n  theme(\n    plot.title    = element_text(face = \"bold\", size = 14),\n    axis.text.y   = element_text(size = 10)\n  )\n\n\n\n\n\nBased on the plot, the most influential variable by far is having 20 to 29 years of driving experience. This predictor stands out with a much higher SHAP value than any other, suggesting that, in this dataset, drivers in this experience group consistently have a strong effect, either increasing or decreasing the model’s predicted probability of a claim. In fact, as shown in the previous odds ratio plot, this predictor is considered a risk reducer.\nThe next most important features are vehicle ownership status and gender, specifically being female. Both of these variables have notable contributions to the model’s predictions, suggesting that whether someone owns their vehicle and their gender play significant roles in how the model assesses risk. Another important observation includes driving a car manufactured before 2015 and being married. Similarly, as we saw in the odds ratio plot, driving a car produced after 2015 appeared to reduce the odds of filing a claim relative to the reference group, which is cars produced before 2015. Following this logic, driving a car before 2015 can be considered a risk contributor. Being married also appears meaningful in the SHAP plot, as it was in the odds ratio plot, though to a lesser extent.\nSeveral other predictors, such as the number of speeding violations, past accidents, education level, and whether the driver has children, have smaller but still measurable impacts on the model’s predictions. Intriguingly, factors like annual mileage, past accidents, driving experience above 30 years, and having children appear to have relatively little to no effect according to the SHAP analysis, even though some of these might be expected to have a noticeable influence.\nThis simply implies that, for this particular model and dataset, experience, car ownership, and certain demographic factors are the dominant drivers of predicted claim risk. Overall, it is plausible to say that these insights can be helpful to raise attention on the characteristics that influence claim risk, especially when we consider the fact that these insights still lie within the traditional assumptions in the insurance industry and can support more focused risk management and pricing strategies ### 3.3 Model Performance and metrics\n\n\nCode\n# Generate predictions on test data\ntest_pred_prob = predict(final_model, newdata = test_data, type = \"response\")\ntest_pred_class = ifelse(test_pred_prob &gt; 0.5, 1, 0)  # Threshold = 0.5\n\n# ROC-AUC\nroc_curve = roc(test_data$OUTCOME, test_pred_prob)\n\n\nSetting levels: control = 0, case = 1\n\n\nSetting direction: controls &lt; cases\n\n\nCode\nauc_value = auc(roc_curve)\nroc_curve\n\n\n\nCall:\nroc.default(response = test_data$OUTCOME, predictor = test_pred_prob)\n\nData: test_pred_prob in 2039 controls (test_data$OUTCOME 0) &lt; 961 cases (test_data$OUTCOME 1).\nArea under the curve: 0.9008\n\n\nCode\n# Plot ROC Curve\nplot(roc_curve, main = \"ROC Curve (Test Data)\", col = \"#2F4F4F\", print.auc = TRUE)\n\n\n\n\n\n\n\nCode\n# Generate confusion matrix\nconf_matrix = confusionMatrix(\n  factor(test_pred_class, levels = c(0, 1)),\n  factor(test_data$OUTCOME, levels = c(0, 1)),\n  positive = \"1\"\n)\n# Convert to dataframe\ncm_df = as.data.frame(conf_matrix$table)\n\n# Create label for each cell\ncm_df = cm_df %&gt;%\n  mutate(Label = case_when(\n    Prediction == 1 & Reference == 1 ~ \"True Positive\",\n    Prediction == 0 & Reference == 1 ~ \"False Negative\",\n    Prediction == 1 & Reference == 0 ~ \"False Positive\",\n    Prediction == 0 & Reference == 0 ~ \"True Negative\"\n  ))\n\n# Plot \nggplot(cm_df, aes(x = Reference, y = Prediction, fill = Freq)) +\n  geom_tile(color = \"white\") +\n  geom_text(aes(label = paste0(Label, \"\\n\", Freq)), color = \"white\", size = 5) +\n  scale_fill_gradient(low = \"#2F4F4F\", high = \"#E74C3C\") +\n  labs(\n    title = \"Confusion Matrix with Labelled Outcomes (Threshold = 0.5)\",\n    x = \"Actual Outcome\",\n    y = \"Predicted Outcome\") +\n  theme_classic()\n\n\n\n\n\nlets see how well the model ranks clients using AUC, a metric mostly used to determine the classification power of the model to differentiate between high vs. low clients. while using confusion metric will help us Show the exact counts of misclassifications (false negatives/positives) which could be helpful in the next step, where we intend to apply it in a practical business related scenarios with simulations."
  }
]